{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cuda:0\nNet(\n  (conv1): Conv2d(1, 6, kernel_size=(10, 4), stride=(1, 1))\n  (conv2): Conv2d(6, 10, kernel_size=(1, 3), stride=(1, 2))\n  (fc1): Linear(in_features=580, out_features=4, bias=True)\n)\nEpoch[1], loss: 1.40070864\nEpoch[2], loss: 1.39497704\nEpoch[3], loss: 1.39153360\nEpoch[4], loss: 1.38835401\nEpoch[5], loss: 1.38532014\nEpoch[6], loss: 1.38232564\nEpoch[7], loss: 1.37930268\nEpoch[8], loss: 1.37619221\nEpoch[9], loss: 1.37300523\nEpoch[10], loss: 1.36978320\nEpoch[11], loss: 1.36646397\nEpoch[12], loss: 1.36300038\nEpoch[13], loss: 1.35931634\nEpoch[14], loss: 1.35538838\nEpoch[15], loss: 1.35114700\nEpoch[16], loss: 1.34657219\nEpoch[17], loss: 1.34165575\nEpoch[18], loss: 1.33637483\nEpoch[19], loss: 1.33067187\nEpoch[20], loss: 1.32455150\nEpoch[21], loss: 1.31809718\nEpoch[22], loss: 1.31117929\nEpoch[23], loss: 1.30391651\nEpoch[24], loss: 1.29633074\nEpoch[25], loss: 1.28837525\nEpoch[26], loss: 1.28021751\nEpoch[27], loss: 1.27187271\nEpoch[28], loss: 1.26337448\nEpoch[29], loss: 1.25483096\nEpoch[30], loss: 1.24619389\nEpoch[31], loss: 1.23761669\nEpoch[32], loss: 1.22893707\nEpoch[33], loss: 1.22028764\nEpoch[34], loss: 1.21153721\nEpoch[35], loss: 1.20269162\nEpoch[36], loss: 1.19367390\nEpoch[37], loss: 1.18435273\nEpoch[38], loss: 1.17459475\nEpoch[39], loss: 1.16405751\nEpoch[40], loss: 1.15236721\nEpoch[41], loss: 1.13906710\nEpoch[42], loss: 1.12370470\nEpoch[43], loss: 1.10591233\nEpoch[44], loss: 1.08556018\nEpoch[45], loss: 1.06363154\nEpoch[46], loss: 1.04213910\nEpoch[47], loss: 1.02152582\nEpoch[48], loss: 1.00147073\nEpoch[49], loss: 0.98200982\nEpoch[50], loss: 0.96319346\nEpoch[51], loss: 0.94471341\nEpoch[52], loss: 0.92620902\nEpoch[53], loss: 0.90772830\nEpoch[54], loss: 0.88965497\nEpoch[55], loss: 0.87185690\nEpoch[56], loss: 0.85389788\nEpoch[57], loss: 0.83629092\nEpoch[58], loss: 0.81894825\nEpoch[59], loss: 0.80194266\nEpoch[60], loss: 0.78524563\nEpoch[61], loss: 0.76888873\nEpoch[62], loss: 0.75299748\nEpoch[63], loss: 0.73754374\nEpoch[64], loss: 0.72284323\nEpoch[65], loss: 0.70855951\nEpoch[66], loss: 0.69458644\nEpoch[67], loss: 0.68129002\nEpoch[68], loss: 0.66863716\nEpoch[69], loss: 0.65663668\nEpoch[70], loss: 0.64508645\nEpoch[71], loss: 0.63372363\nEpoch[72], loss: 0.62282005\nEpoch[73], loss: 0.61252100\nEpoch[74], loss: 0.60280761\nEpoch[75], loss: 0.59307696\nEpoch[76], loss: 0.58381227\nEpoch[77], loss: 0.57469992\nEpoch[78], loss: 0.56591591\nEpoch[79], loss: 0.55735200\nEpoch[80], loss: 0.54906661\nEpoch[81], loss: 0.54102257\nEpoch[82], loss: 0.53317992\nEpoch[83], loss: 0.52578826\nEpoch[84], loss: 0.51825171\nEpoch[85], loss: 0.51100917\nEpoch[86], loss: 0.50391716\nEpoch[87], loss: 0.49680965\nEpoch[88], loss: 0.49012203\nEpoch[89], loss: 0.48350511\nEpoch[90], loss: 0.47684880\nEpoch[91], loss: 0.47059529\nEpoch[92], loss: 0.46432902\nEpoch[93], loss: 0.45850368\nEpoch[94], loss: 0.45260538\nEpoch[95], loss: 0.44684646\nEpoch[96], loss: 0.44114572\nEpoch[97], loss: 0.43568379\nEpoch[98], loss: 0.43002292\nEpoch[99], loss: 0.42451181\nEpoch[100], loss: 0.41924907\nEpoch[101], loss: 0.41397223\nEpoch[102], loss: 0.40915927\nEpoch[103], loss: 0.40389502\nEpoch[104], loss: 0.39890789\nEpoch[105], loss: 0.39394383\nEpoch[106], loss: 0.38927031\nEpoch[107], loss: 0.38434564\nEpoch[108], loss: 0.37959234\nEpoch[109], loss: 0.37452969\nEpoch[110], loss: 0.36993848\nEpoch[111], loss: 0.36524431\nEpoch[112], loss: 0.36060890\nEpoch[113], loss: 0.35608196\nEpoch[114], loss: 0.35141603\nEpoch[115], loss: 0.34677603\nEpoch[116], loss: 0.34235212\nEpoch[117], loss: 0.33779389\nEpoch[118], loss: 0.33339092\nEpoch[119], loss: 0.32876038\nEpoch[120], loss: 0.32446147\nEpoch[121], loss: 0.32009175\nEpoch[122], loss: 0.31613112\nEpoch[123], loss: 0.31196027\nEpoch[124], loss: 0.30775869\nEpoch[125], loss: 0.30362803\nEpoch[126], loss: 0.29945179\nEpoch[127], loss: 0.24438217\nEpoch[128], loss: 0.24291313\nEpoch[129], loss: 0.24162177\nEpoch[130], loss: 0.24048354\nEpoch[131], loss: 0.23947302\nEpoch[132], loss: 0.23856682\nEpoch[133], loss: 0.23776408\nEpoch[134], loss: 0.23704158\nEpoch[135], loss: 0.23638778\nEpoch[136], loss: 0.23579289\nEpoch[137], loss: 0.23524988\nEpoch[138], loss: 0.23475112\nEpoch[139], loss: 0.23428952\nEpoch[140], loss: 0.23386174\nEpoch[141], loss: 0.23346195\nEpoch[142], loss: 0.23308762\nEpoch[143], loss: 0.23273539\nEpoch[144], loss: 0.23240286\nEpoch[145], loss: 0.23208526\nEpoch[146], loss: 0.23178260\nEpoch[147], loss: 0.23149176\nEpoch[148], loss: 0.23121228\nEpoch[149], loss: 0.23094064\nEpoch[150], loss: 0.23067860\nEpoch[151], loss: 0.23042446\nEpoch[152], loss: 0.23017750\nEpoch[153], loss: 0.22993553\nEpoch[154], loss: 0.22969854\nEpoch[155], loss: 0.22946772\nEpoch[156], loss: 0.22924035\nEpoch[157], loss: 0.22901711\nEpoch[158], loss: 0.22879670\nEpoch[159], loss: 0.22858656\nEpoch[160], loss: 0.22837284\npredicted_test:tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), total:202, accuracy:0.9207920792079208, sum:186\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import os \n",
    "\n",
    "\n",
    "def Normlize(Z):\n",
    "    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n",
    "    Zmean = Z.mean(axis=1)\n",
    "    #按列排序\n",
    "    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n",
    "    Zmean = Zmean.reshape(-1, 1)\n",
    "    Z = (Z - Zmean) / (Zmax - Zmin)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def Data_Reading(Normalization=True):\n",
    "    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainset.npy')\n",
    "    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainlabel.npy')\n",
    "    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testset.npy')\n",
    "    test_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testlabel.npy')\n",
    "\n",
    "    # Normalization\n",
    "    train_x_Normed = Normlize(train_x)\n",
    "    test_x_Normed = Normlize(test_x)\n",
    "\n",
    "    # xlsx to tensor\n",
    "    if Normalization:\n",
    "        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n",
    "        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n",
    "        train_y = torch.from_numpy(train_y).type(torch.int64)\n",
    "        test_y = torch.from_numpy(test_y).type(torch.int64)\n",
    "\n",
    "    else:\n",
    "        train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "        test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "        train_y = torch.from_numpy(train_y).type(torch.int64)\n",
    "        test_y = torch.from_numpy(test_y).type(torch.int64)\n",
    "    # reshape\n",
    "    train_x = train_x.view(408, 1, 10, 120)\n",
    "    test_x = test_x.view(202, 1, 10, 120)\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,(10,4),stride=(1,1))#(10 - 10)/1 + 1 = 1   (120 - 4)/1 + 1 = 117 \n",
    "        self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,2))#(1 - 1)/1 + 1 = 1   (117 - 3)/2 + 1 = 58 116/2= 58\n",
    "        self.fc1 = nn.Linear(10*1*58,4)\n",
    "\n",
    "       \n",
    "        # self.conv1 = nn.Conv2d(1,6,(10,4),stride=(1,2))#59\n",
    "        # self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,1))#57      \n",
    "        # self.fc1 = nn.Linear(10*1*57,4)\n",
    "\n",
    "        # self.fc2 = nn.Linear(342,4)\n",
    "        # self.fc3 = nn.Linear(85,4)\n",
    "        # self.fc4 = nn.Linear(64,4)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # x = F.lrelu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.logsigmoid(self.fc1(x))\n",
    "        # x = F.logsigmoid(self.fc2(x))\n",
    "        # x = F.relu(self.fc3(x))\n",
    "        x = self.fc1(x)\n",
    "        # return x\n",
    "        return x#F.softmax(x,dim=0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    cnn = Net()\n",
    "    print(cnn)\n",
    "    cnn.to(device)\n",
    "\n",
    "    #sgd -> stochastic gradient descent\n",
    "    optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.8)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_x, test_x, train_y, test_y = Data_Reading(Normalization=1)\n",
    "    train_y = train_y.squeeze()\n",
    "    test_y = test_y.squeeze()\n",
    "    train_x = train_x.to(device)\n",
    "    test_x = test_x.to(device)\n",
    "    train_y = train_y.to(device)\n",
    "    test_y = test_y.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    #test\n",
    "    batch_size = 17\n",
    "    step = 10\n",
    "    for epoch in range(160):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0,(int)(len(train_x)/batch_size)):\n",
    "            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n",
    "            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n",
    "            out = cnn(t_x)\n",
    "            #forward\n",
    "            loss = loss_func(out, t_y)\n",
    "            #梯度初始化为零\n",
    "            optimizer.zero_grad()\n",
    "             \n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # predicted1 = torch.max(out1.data, 1)[1]\n",
    "   \n",
    "        running_loss = running_loss / 24\n",
    "\n",
    "        if (running_loss < 0.3) :\n",
    "            optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.6)\n",
    "        elif (running_loss < 0.2) :\n",
    "            optimizer = optim.SGD(cnn.parameters(), lr=0.00001, momentum=0.4)\n",
    "\n",
    "        # if (epoch + 1) % step == 0:\n",
    "        print('Epoch[{}], loss: {:.8f}'.format(epoch + 1, running_loss))\n",
    "    \n",
    "    #class\n",
    "    sum = 0\n",
    "\n",
    "    te_x = Variable(test_x)\n",
    "    te_y = Variable(test_y)\n",
    "    out1 = cnn(te_x)\n",
    "    predicted_test = torch.max(out1.data, 1)[1]#.data.squeeze()\n",
    "    total = te_y.size(0)\n",
    "\n",
    "    for j in range(te_y.size(0)):\n",
    "        if predicted_test[j] == te_y[j]:\n",
    "            sum += 1\n",
    "\n",
    "    print('predicted_test:{}, total:{}, accuracy:{}, sum:{}'.format(predicted_test, total, sum / total, sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}