{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"    [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 2 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 3 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 4 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 5 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 6 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 7 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 8 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 9 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 10 tensor(0.2533, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13100.2500],\n        [12949.9541],\n        [12951.9863],\n        [12877.1553],\n        [12862.7002],\n        [12957.6562],\n        [12899.8301],\n        [12819.3887],\n        [12779.0713],\n        [13019.3174],\n        [12812.6748],\n        [13104.4150],\n        [12754.3242],\n        [13034.5381],\n        [13195.5039],\n        [13055.5967],\n        [13090.3125],\n        [13054.5029],\n        [13014.7715],\n        [12675.2881],\n        [12963.5791],\n        [12788.3096],\n        [12919.8096],\n        [13271.3115],\n        [13257.1348],\n        [13240.7783],\n        [13188.6299],\n        [13207.2793],\n        [15652.3623],\n        [16082.7656],\n        [15753.4023],\n        [16774.6270],\n        [16058.9209],\n        [14247.3877],\n        [14110.4531],\n        [13957.0322],\n        [14164.0039],\n        [13035.3662],\n        [12860.0381],\n        [12941.6621],\n        [12889.9639],\n        [12877.9932],\n        [13945.9639],\n        [13891.1846],\n        [13955.4004],\n        [13818.3164],\n        [13889.1367],\n        [12996.8154],\n        [13138.8047],\n        [12910.6064],\n        [13082.1992],\n        [13171.6416],\n        [13232.0664],\n        [13258.5850],\n        [13079.1865],\n        [13064.6494],\n        [13190.9727],\n        [13699.1943],\n        [14559.8496],\n        [14567.6973],\n        [14561.9785],\n        [13232.9307],\n        [13123.9316],\n        [13057.7969],\n        [12995.8926],\n        [12913.4512],\n        [12785.4229],\n        [12722.0742],\n        [12771.3643],\n        [13020.5605],\n        [12399.3604],\n        [13039.7559],\n        [12385.3311],\n        [12400.2061],\n        [12697.2471],\n        [12633.1465],\n        [12606.1455],\n        [12490.5537],\n        [13701.3213],\n        [14668.4873],\n        [14767.3164],\n        [14628.8320],\n        [14607.7471],\n        [13163.6025],\n        [13117.8975],\n        [13099.1592],\n        [13040.9502],\n        [13077.5713],\n        [13966.7188],\n        [13871.5576],\n        [13852.1191],\n        [13659.3320],\n        [13653.5361],\n        [12606.4375],\n        [12030.3525]], device='cuda:0', grad_fn=<MulBackward0>)\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","def Data_Reading(Normalization=True):\n","    # Read the xlsx\n","    '''\n","    //读取excel的值\n","    train_x = pd.read_excel( \"trainingset.xlsx\", 'Input',header = None)\n","    test_x = pd.read_excel(\"oilsset.xlsx\",'oils',header = None)\n","    test_z = pd.read_excel(\"newodorset.xlsx\",'new',header = None)\n","    train_y = pd.read_excel(\"trainingy.xlsx\",'Output',header = None)\n","    testy1 = pd.read_excel(\"oilsy.xlsx\",'oy',header = None)\n","    testy2 = pd.read_excel(\"newy.xlsx\",'ny',header = None)\n","    \n","    //使用前一个观察值填充 \n","    train_x = train_x.fillna(method='ffill')\n","    test_x = test_x.fillna(method='ffill')\n","    text_z = test_z.fillna(method='ffill')\n","    train_y = train_y.fillna(method='ffill')\n","\n","    np.save('trainingset.npy', train_x)\n","    np.save('oilsset.npy',test_x)\n","    np.save('newodorset.npy', test_z)\n","    np.save('trainingy.npy', train_y)\n","    np.save('testy1.npy', testy1)\n","    np.save('testy2.npy', testy2)\n","    '''\n","    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\trainingset.npy')\n","    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\oilsset.npy')\n","    test_z = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\newodorset.npy')\n","    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\trainingy.npy')\n","    testy1 = np.load(\"F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\testy1.npy\")\n","    testy2 = np.load(\"F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\testy2.npy\")\n"," \n","\n","    # Normalization\n","    train_x_Normed = Normlize(train_x)\n","    test_x_Normed = Normlize(test_x)\n","    test_z_Normed = Normlize(test_z)\n","    train_y = train_y / 10000\n","    testy1 = testy1 / 10000\n","    testy2 = testy2 / 10000\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n","        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n","        test_z = torch.from_numpy(test_z_Normed).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.cuda.FloatTensor)\n","        testy1 = torch.from_numpy(testy1).type(torch.cuda.FloatTensor)\n","        testy2 = torch.from_numpy(testy2).type(torch.cuda.FloatTensor)\n","\n","    else:\n","        train_x = torch.from_numpy(train_x).type(torch.cuda.FloatTensor)\n","        test_x = torch.from_numpy(test_x).type(torch.cuda.FloatTensor)\n","        test_z = torch.from_numpy(test_z).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.cuda.FloatTensor)\n","        testy1 = torch.from_numpy(testy1).type(torch.cuda.FloatTensor)\n","        testy2 = torch.from_numpy(testy2).type(torch.cuda.FloatTensor)\n","\n","\n","    # reshape\n","    train_x = train_x.view(238, 1, 16, 250)\n","    test_x = test_x.view(108, 1, 16, 250)\n","    test_z = test_z.view(95, 1, 16, 250)\n","    return train_x, test_x, test_z, train_y,testy1,testy2\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1,6,(16,4),stride=(1,3))\n","        self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,2))\n","        #self.conv3 = nn.Conv2d(10,14,(1,4),stride=(1,2))\n","        self.fc = nn.Linear(10*1*41,1)\n","        \n","\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.01)\n","\n","\n","             \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))#（238,6,1,124）\n","        x = F.relu(self.conv2(x))#（238,16,1,23）\n","        #x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1) # flatten the tensor\n","        x = self.fc(x)\n","        return x\n","\n","#training\n","batch_size = 14\n","def train(train_x,train_y,step=20):\n","    for epoch in range(160):\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            t_x = t_x.to(device)\n","            t_y = t_y.to(device)\n","            out = cnn(t_x)\n","            #forward\n","            #loss_func = nn.MSELoss() 均方损失函数  loss(x(i),y(i)) = (x(i) - y(i))^2\n","            loss = loss_func(out, t_y)\n","            #梯度初始化为零\n","            optimizer.zero_grad()\n","             \n","            #backward\n","            loss.backward()\n","            optimizer.step()\n","        if (epoch + 1) % step == 0:\n","            print('Epoch[{}/{}], loss: {:.12f},'.format(epoch + 1,160, loss.item()))\n","        \n","            \n","#predicting\n","def predict(test_x,testy1):\n","    for epoch in range(30):\n","        te_x = Variable(test_x)\n","        tey1 = Variable(testy1)\n","        out1 = cnn(te_x)\n","        loss1 = loss_func(out1, tey1)\n","        out1 = out1 * 10000\n","        #print('Epoch[{}/{}], loss1: {:.12f},'.format(epoch + 1, 30, loss1.item()))\n","        print('epoch, loss1:, out1', epoch + 1, loss1, out1)\n","\n","def newodor(test_z,testy2):\n","    for epoch in range(10):\n","        te_z = Variable(test_z)\n","        tey2 = Variable(testy2)\n","        out2 = cnn(te_z)\n","        loss2 = loss_func(out2, tey2)\n","        out2 = out2 * 10000\n","        #print('Epoch[{}/{}], loss2: {:.12f},'.format(epoch + 1, 10, loss2.item()))\n","        print('epoch, loss2:, out2', epoch + 1, loss2, out2)\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","    \n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.8)\n","    loss_func = nn.MSELoss()\n","\n","    train_x, test_x, test_z, train_y,testy1,testy2 = Data_Reading(Normalization=True)\n","    train(train_x, train_y, step=20)\n","    predict(test_x,testy1)\n","    newodor(test_z,testy2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}