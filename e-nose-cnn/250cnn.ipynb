{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"    [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 2 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 3 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 4 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 5 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 6 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 7 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 8 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 9 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 10 tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[12655.3008],\n        [12579.0410],\n        [12613.2920],\n        [12611.4072],\n        [12611.1270],\n        [13159.8369],\n        [13051.1221],\n        [12996.7686],\n        [12970.5723],\n        [13371.7803],\n        [13177.2842],\n        [13263.8975],\n        [13154.3945],\n        [13228.3027],\n        [12849.3730],\n        [12710.1377],\n        [12649.4082],\n        [12632.4736],\n        [12784.1338],\n        [14016.5869],\n        [14109.5508],\n        [13996.3047],\n        [14309.4668],\n        [12426.3252],\n        [12350.4336],\n        [12294.4365],\n        [12280.8896],\n        [12366.1816],\n        [16118.0791],\n        [14947.6055],\n        [14921.1787],\n        [15037.6650],\n        [14826.9697],\n        [16029.1211],\n        [16082.8721],\n        [16140.4277],\n        [15998.5801],\n        [12719.4736],\n        [12592.4961],\n        [12558.7217],\n        [12565.7881],\n        [12527.3369],\n        [13572.5234],\n        [13465.9180],\n        [13550.0381],\n        [13328.6309],\n        [13472.3398],\n        [12142.4658],\n        [12412.2266],\n        [11809.2764],\n        [12283.8418],\n        [12238.6660],\n        [11561.9199],\n        [11461.7744],\n        [11447.4873],\n        [11487.7041],\n        [11442.9092],\n        [14985.2607],\n        [16031.0146],\n        [16113.0176],\n        [16138.5312],\n        [14628.8779],\n        [14585.3965],\n        [14458.9902],\n        [11244.3779],\n        [11000.3467],\n        [11095.9111],\n        [11184.4746],\n        [10977.7842],\n        [11357.3457],\n        [10856.3457],\n        [11389.9180],\n        [10915.7412],\n        [10943.4775],\n        [12249.5127],\n        [12117.5830],\n        [12044.0127],\n        [12023.5088],\n        [14585.1123],\n        [15530.3574],\n        [15607.7432],\n        [15508.7002],\n        [15494.1797],\n        [12081.6816],\n        [12025.8232],\n        [11982.4688],\n        [11922.5801],\n        [11980.8340],\n        [12391.9629],\n        [12266.3115],\n        [12249.9619],\n        [12167.2676],\n        [12178.6318],\n        [14272.7344],\n        [13629.7568]], device='cuda:0', grad_fn=<MulBackward0>)\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","def Data_Reading(Normalization=True):\n","    # Read the xlsx\n","    '''\n","    //读取excel的值\n","    train_x = pd.read_excel( \"trainingset.xlsx\", 'Input',header = None)\n","    test_x = pd.read_excel(\"oilsset.xlsx\",'oils',header = None)\n","    test_z = pd.read_excel(\"newodorset.xlsx\",'new',header = None)\n","    train_y = pd.read_excel(\"trainingy.xlsx\",'Output',header = None)\n","    testy1 = pd.read_excel(\"oilsy.xlsx\",'oy',header = None)\n","    testy2 = pd.read_excel(\"newy.xlsx\",'ny',header = None)\n","    \n","    //使用前一个观察值填充 \n","    train_x = train_x.fillna(method='ffill')\n","    test_x = test_x.fillna(method='ffill')\n","    text_z = test_z.fillna(method='ffill')\n","    train_y = train_y.fillna(method='ffill')\n","\n","    np.save('trainingset.npy', train_x)\n","    np.save('oilsset.npy',test_x)\n","    np.save('newodorset.npy', test_z)\n","    np.save('trainingy.npy', train_y)\n","    np.save('testy1.npy', testy1)\n","    np.save('testy2.npy', testy2)\n","    '''\n","    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\trainingset.npy')\n","    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\oilsset.npy')\n","    test_z = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\newodorset.npy')\n","    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\trainingy.npy')\n","    testy1 = np.load(\"F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\testy1.npy\")\n","    testy2 = np.load(\"F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\testy2.npy\")\n"," \n","\n","    # Normalization\n","    train_x_Normed = Normlize(train_x)\n","    test_x_Normed = Normlize(test_x)\n","    test_z_Normed = Normlize(test_z)\n","    train_y = train_y / 10000\n","    testy1 = testy1 / 10000\n","    testy2 = testy2 / 10000\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n","        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n","        test_z = torch.from_numpy(test_z_Normed).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.cuda.FloatTensor)\n","        testy1 = torch.from_numpy(testy1).type(torch.cuda.FloatTensor)\n","        testy2 = torch.from_numpy(testy2).type(torch.cuda.FloatTensor)\n","\n","    else:\n","        train_x = torch.from_numpy(train_x).type(torch.cuda.FloatTensor)\n","        test_x = torch.from_numpy(test_x).type(torch.cuda.FloatTensor)\n","        test_z = torch.from_numpy(test_z).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.cuda.FloatTensor)\n","        testy1 = torch.from_numpy(testy1).type(torch.cuda.FloatTensor)\n","        testy2 = torch.from_numpy(testy2).type(torch.cuda.FloatTensor)\n","\n","\n","    # reshape\n","    train_x = train_x.view(238, 1, 16, 250)\n","    test_x = test_x.view(108, 16, 1, 250)\n","    test_z = test_z.view(95, 16, 1, 250)\n","    return train_x, test_x, test_z, train_y,testy1,testy2\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1,6,(16,4),stride=(1,3))#(250-4)/3+1 = 83\n","        self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,2))#\n","        #self.conv3 = nn.Conv2d(10,14,(1,4),stride=(1,2))\n","        self.fc = nn.Linear(943,1)\n","        \n","\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.01)\n","\n","\n","             \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))#（238,6,1,124）\n","        x = F.relu(self.conv2(x))#（238,16,1,23）\n","        #x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1) # flatten the tensor\n","        x = self.fc(x)\n","        return x\n","\n","#training\n","batch_size = 14\n","def train(train_x,train_y,step=20):\n","    for epoch in range(160):\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            t_x = t_x.to(device)\n","            t_y = t_y.to(device)\n","            out = cnn(t_x)\n","            #forward\n","            #loss_func = nn.MSELoss() 均方损失函数  loss(x(i),y(i)) = (x(i) - y(i))^2\n","            loss = loss_func(out, t_y)\n","            #梯度初始化为零\n","            optimizer.zero_grad()\n","             \n","            #backward\n","            loss.backward()\n","            optimizer.step()\n","        if (epoch + 1) % step == 0:\n","            print('Epoch[{}/{}], loss: {:.12f},'.format(epoch + 1,160, loss.item()))\n","        \n","            \n","#predicting\n","def predict(test_x,testy1):\n","    for epoch in range(30):\n","        te_x = Variable(test_x)\n","        tey1 = Variable(testy1)\n","        out1 = cnn(te_x)\n","        loss1 = loss_func(out1, tey1)\n","        out1 = out1 * 10000\n","        #print('Epoch[{}/{}], loss1: {:.12f},'.format(epoch + 1, 30, loss1.item()))\n","        print('epoch, loss1:, out1', epoch + 1, loss1, out1)\n","\n","def newodor(test_z,testy2):\n","    for epoch in range(10):\n","        te_z = Variable(test_z)\n","        tey2 = Variable(testy2)\n","        out2 = cnn(te_z)\n","        loss2 = loss_func(out2, tey2)\n","        out2 = out2 * 10000\n","        #print('Epoch[{}/{}], loss2: {:.12f},'.format(epoch + 1, 10, loss2.item()))\n","        print('epoch, loss2:, out2', epoch + 1, loss2, out2)\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","    \n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.8)\n","    loss_func = nn.MSELoss()\n","\n","    train_x, test_x, test_z, train_y,testy1,testy2 = Data_Reading(Normalization=True)\n","    train(train_x, train_y, step=20)\n","    predict(test_x,testy1)\n","    newodor(test_z,testy2)\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[-1.2000e+00 -1.2000e+00 -1.2000e+00 ...  6.8000e+00  6.8000e+00\n   6.8000e+00]\n [-2.3333e+00 -1.3333e+00 -2.3333e+00 ...  3.6667e+01  3.6667e+01\n   3.6667e+01]\n [-3.6250e+00 -3.6250e+00 -3.6250e+00 ...  4.6375e+01  4.7375e+01\n   4.6375e+01]\n ...\n [-1.4000e+00 -4.0000e-01 -4.0000e-01 ...  3.0060e+02  2.9860e+02\n   2.9660e+02]\n [ 0.0000e+00  0.0000e+00  0.0000e+00 ...  1.6100e+02  1.6000e+02\n   1.5900e+02]\n [ 0.0000e+00  0.0000e+00  0.0000e+00 ...  9.6200e+02  9.5600e+02\n   9.4900e+02]]\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","train_x = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\trainingset.npy')\n","testy2 = np.load(\"F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\testy2.npy\")\n","print(train_x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}