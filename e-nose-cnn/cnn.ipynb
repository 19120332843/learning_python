{"cells":[{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"=======================================\ntotal_train:525, accuracy:0.8857142857142857, sum:465\nEpoch[69], loss: 0.34059293\ntotal:175, accuracy:0.64, sum:112, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8895238095238095, sum:467\nEpoch[70], loss: 0.33156929\ntotal:175, accuracy:0.6342857142857142, sum:111, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.88, sum:462\nEpoch[71], loss: 0.32692434\ntotal:175, accuracy:0.6628571428571428, sum:116, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8819047619047619, sum:463\nEpoch[72], loss: 0.32012535\ntotal:175, accuracy:0.6514285714285715, sum:114, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8990476190476191, sum:472\nEpoch[73], loss: 0.30880069\ntotal:175, accuracy:0.6514285714285715, sum:114, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8971428571428571, sum:471\nEpoch[74], loss: 0.28740732\ntotal:175, accuracy:0.6285714285714286, sum:110, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8952380952380953, sum:470\nEpoch[75], loss: 0.27335242\ntotal:175, accuracy:0.64, sum:112, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8933333333333333, sum:469\nEpoch[76], loss: 0.26564715\ntotal:175, accuracy:0.6342857142857142, sum:111, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8895238095238095, sum:467\nEpoch[77], loss: 0.25158363\ntotal:175, accuracy:0.6342857142857142, sum:111, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8857142857142857, sum:465\nEpoch[78], loss: 0.24507011\ntotal:175, accuracy:0.6457142857142857, sum:113, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8914285714285715, sum:468\nEpoch[79], loss: 0.24162933\ntotal:175, accuracy:0.6514285714285715, sum:114, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9104761904761904, sum:478\nEpoch[80], loss: 0.24014565\ntotal:175, accuracy:0.6685714285714286, sum:117, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9161904761904762, sum:481\nEpoch[81], loss: 0.26267813\ntotal:175, accuracy:0.6628571428571428, sum:116, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.8876190476190476, sum:466\nEpoch[82], loss: 0.25770001\ntotal:175, accuracy:0.6628571428571428, sum:116, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9066666666666666, sum:476\nEpoch[83], loss: 0.29153053\ntotal:175, accuracy:0.68, sum:119, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9161904761904762, sum:481\nEpoch[84], loss: 0.22332338\ntotal:175, accuracy:0.6628571428571428, sum:116, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.92, sum:483\nEpoch[85], loss: 0.20938222\ntotal:175, accuracy:0.6571428571428571, sum:115, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9314285714285714, sum:489\nEpoch[86], loss: 0.20579729\ntotal:175, accuracy:0.68, sum:119, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9314285714285714, sum:489\nEpoch[87], loss: 0.20582335\ntotal:175, accuracy:0.6857142857142857, sum:120, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9295238095238095, sum:488\nEpoch[88], loss: 0.19031032\ntotal:175, accuracy:0.6914285714285714, sum:121, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9352380952380952, sum:491\nEpoch[89], loss: 0.17814774\ntotal:175, accuracy:0.68, sum:119, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.9219047619047619, sum:484\nEpoch[90], loss: 0.18480863\ntotal:175, accuracy:0.6914285714285714, sum:121, max=0.6914285714285714, maxepoch=59\n=============================================================================\ntotal_train:525, accuracy:0.940952380952381, sum:494\nEpoch[91], loss: 0.16898681\ntotal:175, accuracy:0.6971428571428572, sum:122, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.940952380952381, sum:494\nEpoch[92], loss: 0.18636774\ntotal:175, accuracy:0.68, sum:119, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9428571428571428, sum:495\nEpoch[93], loss: 0.18513918\ntotal:175, accuracy:0.68, sum:119, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9561904761904761, sum:502\nEpoch[94], loss: 0.16590729\ntotal:175, accuracy:0.6914285714285714, sum:121, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9428571428571428, sum:495\nEpoch[95], loss: 0.16680088\ntotal:175, accuracy:0.6857142857142857, sum:120, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9314285714285714, sum:489\nEpoch[96], loss: 0.16519247\ntotal:175, accuracy:0.6685714285714286, sum:117, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9333333333333333, sum:490\nEpoch[97], loss: 0.16411371\ntotal:175, accuracy:0.6628571428571428, sum:116, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.939047619047619, sum:493\nEpoch[98], loss: 0.15718530\ntotal:175, accuracy:0.6685714285714286, sum:117, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.940952380952381, sum:494\nEpoch[99], loss: 0.14786398\ntotal:175, accuracy:0.6857142857142857, sum:120, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9257142857142857, sum:486\nEpoch[100], loss: 0.14140180\ntotal:175, accuracy:0.6628571428571428, sum:116, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9561904761904761, sum:502\nEpoch[101], loss: 0.15332630\ntotal:175, accuracy:0.68, sum:119, max=0.6971428571428572, maxepoch=91\n=============================================================================\ntotal_train:525, accuracy:0.9561904761904761, sum:502\nEpoch[102], loss: 0.15094542\ntotal:175, accuracy:0.7257142857142858, sum:127, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9447619047619048, sum:496\nEpoch[103], loss: 0.13412597\ntotal:175, accuracy:0.7085714285714285, sum:124, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9352380952380952, sum:491\nEpoch[104], loss: 0.12880518\ntotal:175, accuracy:0.6857142857142857, sum:120, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9485714285714286, sum:498\nEpoch[105], loss: 0.12984559\ntotal:175, accuracy:0.6742857142857143, sum:118, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9561904761904761, sum:502\nEpoch[106], loss: 0.12967661\ntotal:175, accuracy:0.7142857142857143, sum:125, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9580952380952381, sum:503\nEpoch[107], loss: 0.13226199\ntotal:175, accuracy:0.6857142857142857, sum:120, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.8952380952380953, sum:470\nEpoch[108], loss: 0.16155100\ntotal:175, accuracy:0.6457142857142857, sum:113, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9295238095238095, sum:488\nEpoch[109], loss: 0.16626733\ntotal:175, accuracy:0.6742857142857143, sum:118, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.96, sum:504\nEpoch[110], loss: 0.13395947\ntotal:175, accuracy:0.6857142857142857, sum:120, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9257142857142857, sum:486\nEpoch[111], loss: 0.12638718\ntotal:175, accuracy:0.6457142857142857, sum:113, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9485714285714286, sum:498\nEpoch[112], loss: 0.13130021\ntotal:175, accuracy:0.7257142857142858, sum:127, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9542857142857143, sum:501\nEpoch[113], loss: 0.14673137\ntotal:175, accuracy:0.7142857142857143, sum:125, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9485714285714286, sum:498\nEpoch[114], loss: 0.11440349\ntotal:175, accuracy:0.6971428571428572, sum:122, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9447619047619048, sum:496\nEpoch[115], loss: 0.12459769\ntotal:175, accuracy:0.6971428571428572, sum:122, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9447619047619048, sum:496\nEpoch[116], loss: 0.14292206\ntotal:175, accuracy:0.7028571428571428, sum:123, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.940952380952381, sum:494\nEpoch[117], loss: 0.13153204\ntotal:175, accuracy:0.7028571428571428, sum:123, max=0.7257142857142858, maxepoch=102\n=============================================================================\ntotal_train:525, accuracy:0.9619047619047619, sum:505\nEpoch[118], loss: 0.14195324\ntotal:175, accuracy:0.7657142857142857, sum:134, max=0.7657142857142857, maxepoch=118\n=============================================================================\ntotal_train:525, accuracy:0.9542857142857143, sum:501\nEpoch[119], loss: 0.12180605\ntotal:175, accuracy:0.7657142857142857, sum:134, max=0.7657142857142857, maxepoch=118\n=============================================================================\ntotal_train:525, accuracy:0.9466666666666667, sum:497\nEpoch[120], loss: 0.12806143\ntotal:175, accuracy:0.7828571428571428, sum:137, max=0.7828571428571428, maxepoch=120\n=============================================================================\ntotal_train:525, accuracy:0.96, sum:504\nEpoch[121], loss: 0.16298420\ntotal:175, accuracy:0.76, sum:133, max=0.7828571428571428, maxepoch=120\n=============================================================================\ntotal_train:525, accuracy:0.96, sum:504\nEpoch[122], loss: 0.11407777\ntotal:175, accuracy:0.7657142857142857, sum:134, max=0.7828571428571428, maxepoch=120\n=============================================================================\ntotal_train:525, accuracy:0.9638095238095238, sum:506\nEpoch[123], loss: 0.10668242\ntotal:175, accuracy:0.76, sum:133, max=0.7828571428571428, maxepoch=120\n=============================================================================\ntotal_train:525, accuracy:0.9638095238095238, sum:506\nEpoch[124], loss: 0.10075647\ntotal:175, accuracy:0.7885714285714286, sum:138, max=0.7885714285714286, maxepoch=124\n=============================================================================\ntotal_train:525, accuracy:0.9695238095238096, sum:509\nEpoch[125], loss: 0.10107526\ntotal:175, accuracy:0.7885714285714286, sum:138, max=0.7885714285714286, maxepoch=124\n=============================================================================\ntotal_train:525, accuracy:0.9733333333333334, sum:511\nEpoch[126], loss: 0.09094464\ntotal:175, accuracy:0.7942857142857143, sum:139, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9714285714285714, sum:510\nEpoch[127], loss: 0.07966047\ntotal:175, accuracy:0.7885714285714286, sum:138, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9809523809523809, sum:515\nEpoch[128], loss: 0.07361297\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9485714285714286, sum:498\nEpoch[129], loss: 0.25473531\ntotal:175, accuracy:0.7142857142857143, sum:125, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9619047619047619, sum:505\nEpoch[130], loss: 0.11106681\ntotal:175, accuracy:0.7485714285714286, sum:131, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9695238095238096, sum:509\nEpoch[131], loss: 0.09326159\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9809523809523809, sum:515\nEpoch[132], loss: 0.07966901\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9752380952380952, sum:512\nEpoch[133], loss: 0.06887738\ntotal:175, accuracy:0.7828571428571428, sum:137, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.979047619047619, sum:514\nEpoch[134], loss: 0.06969519\ntotal:175, accuracy:0.7942857142857143, sum:139, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9828571428571429, sum:516\nEpoch[135], loss: 0.06217965\ntotal:175, accuracy:0.7828571428571428, sum:137, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9866666666666667, sum:518\nEpoch[136], loss: 0.06373511\ntotal:175, accuracy:0.7885714285714286, sum:138, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9847619047619047, sum:517\nEpoch[137], loss: 0.06276667\ntotal:175, accuracy:0.7828571428571428, sum:137, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9828571428571429, sum:516\nEpoch[138], loss: 0.05911537\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9847619047619047, sum:517\nEpoch[139], loss: 0.05499476\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9885714285714285, sum:519\nEpoch[140], loss: 0.05189779\ntotal:175, accuracy:0.7828571428571428, sum:137, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9885714285714285, sum:519\nEpoch[141], loss: 0.05475771\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9923809523809524, sum:521\nEpoch[142], loss: 0.05065227\ntotal:175, accuracy:0.7714285714285715, sum:135, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9904761904761905, sum:520\nEpoch[143], loss: 0.04434824\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9904761904761905, sum:520\nEpoch[144], loss: 0.04122362\ntotal:175, accuracy:0.7771428571428571, sum:136, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9904761904761905, sum:520\nEpoch[145], loss: 0.04074886\ntotal:175, accuracy:0.7714285714285715, sum:135, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9904761904761905, sum:520\nEpoch[146], loss: 0.03519662\ntotal:175, accuracy:0.7657142857142857, sum:134, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9885714285714285, sum:519\nEpoch[147], loss: 0.03276420\ntotal:175, accuracy:0.76, sum:133, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9904761904761905, sum:520\nEpoch[148], loss: 0.02802532\ntotal:175, accuracy:0.7714285714285715, sum:135, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9904761904761905, sum:520\nEpoch[149], loss: 0.02557810\ntotal:175, accuracy:0.7657142857142857, sum:134, max=0.7942857142857143, maxepoch=126\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[150], loss: 0.02328222\ntotal:175, accuracy:0.7714285714285715, sum:135, max=0.7942857142857143, maxepoch=126\n=============================================================================\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import os \n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","def Normlize2(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmin) / (Zmax - Zmin)\n","    return Z\n","\n","\n","def Data_Reading(Normalization=True):\n","    trainset = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\trainset2.npy')\n","    trainlabel = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\trainlabel2.npy')\n","    testset = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\testset2.npy')\n","    testlabel = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\testlabel2.npy')\n","\n","    # Normalization\n","    trainset = Normlize(trainset)\n","    testset = Normlize(testset)\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(trainset).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(trainlabel).type(torch.int64)\n","        test_x = torch.from_numpy(testset).type(torch.cuda.FloatTensor)\n","        test_y = torch.from_numpy(testlabel).type(torch.int64)\n","    else:\n","        data = torch.from_numpy(data).type(torch.FloatTensor)\n","        label = torch.from_numpy(label).type(torch.int64)\n","\n","    # reshape\n","    train_x = train_x.view(525, 10, 1, 120)\n","    train_x2 = train_x.view(525, 1, 10, 120)\n","    test_x = test_x.view(175, 10, 1, 120)\n","    test_x2 = test_x.view(175, 1, 10, 120)\n","    \n","    # data = data.cpu().numpy()\n","    # label = label.numpy()    \n","    # train_x, test_x, train_y, test_y = train_test_split(data, label, test_size=0.3) \n","    # train_x = torch.from_numpy(train_x).type(torch.cuda.FloatTensor)\n","    # test_x = torch.from_numpy(test_x).type(torch.cuda.FloatTensor)\n","    # train_y = torch.from_numpy(train_y).type(torch.int64)\n","    # test_y = torch.from_numpy(test_y).type(torch.int64)\n","    \n","\n","    permutation = np.random.permutation(train_y.shape[0])\n","    train_x = train_x[permutation, :, :, :]\n","    print('---------------------------------------------------------------------------')\n","    train_y = train_y[permutation]\n","    return train_x, test_x, train_y, test_y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # self.conv1 = nn.Conv2d(1,3,(1,3),stride=(1,1))#（10-1）/1+1 = 10 （120 - 3）/1 + 1 = 118\n","        # self.conv2 = nn.Conv2d(3,6,(10,1),stride=(1,1))#1   118\n","        # self.conv3 = nn.Conv2d(6,10,(1,4),stride=(1,2))\n","        # self.fc1 = nn.Linear(10*1*58,7)\n","        self.conv1 = nn.Conv2d(10,5,(1,3),stride=(1,1))#10 :（120 - 3）/1 + 1 = 118  \n","        self.conv2 = nn.Conv2d(5,10,(1,3),stride=(1,1))#1 : (118-3)/1+1 = 116\n","        self.conv3 = nn.Conv2d(10,15,(1,3),stride=(1,1))#1 : (116-3)/1+1= 114\n","        self.fc1 = nn.Linear(15*1*114,427)#427\n","        self.fc2 = nn.Linear(427,7)\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.001)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x#\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n","\n","    print(device)\n","\n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    lrr = 0.01\n","    optimizer = optim.SGD(cnn.parameters(), lr=lrr, momentum=0.8)#\n","    loss_func = nn.CrossEntropyLoss()#CrossEntropyLoss()\n","\n","    train_x, test_x, train_y, test_y = Data_Reading(Normalization=1)\n","    train_y = train_y.squeeze()\n","    test_y = test_y.squeeze()\n","    train_x = train_x.to(device)\n","    test_x = test_x.to(device)\n","    train_y = train_y.to(device)\n","    test_y = test_y.to(device)\n","\n","    \n","\n","    #train\n","    sum = 0\n","    max = 0\n","    \n","    batch_size = 21\n","    tr_x = Variable(train_x)\n","    tr_y = Variable(train_y)\n","    for epoch in range(150):\n","        running_loss = 0.0\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            out = cnn(t_x)\n","            loss = loss_func(out, t_y)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","   \n","        running_loss = running_loss / 25\n","\n","        out = cnn(tr_x)\n","        predicted_train = torch.max(out.data, 1)[1]\n","        total_train = tr_y.size(0)\n","        for j in range(tr_y.size(0)):\n","            if predicted_train[j] == tr_y[j]:\n","                sum += 1\n","        \n","        print('total_train:{}, accuracy:{}, sum:{}'.format(total_train, sum / total_train, sum))\n","        sum = 0\n","\n","        if (sum / total_train > 0.90) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=lrr/10, momentum=0.8/4)\n","        elif (sum / total_train > 0.95) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=lrr/10/10, momentum=0)#momentum=0\n","\n","\n","        print('Epoch[{}], loss: {:.8f}'.format(epoch + 1, running_loss))\n","    \n","    \n","    #test\n","        te_x = Variable(test_x)\n","        te_y = Variable(test_y)\n","        out1 = cnn(te_x)\n","        predicted_test = torch.max(out1.data, 1)[1]#.data.squeeze()\n","        total = te_y.size(0)\n","\n","        for j in range(te_y.size(0)):\n","            if predicted_test[j] == te_y[j]:\n","                sum += 1\n","        \n","        if(max < sum/total):\n","            max = sum/total\n","            maxepoch = epoch + 1\n","        print('total:{}, accuracy:{}, sum:{}, max={}, maxepoch={}'.format(total, sum / total, sum, max, maxepoch))\n","        print('=============================================================================')\n","        sum = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"10.714285714285714\n"}],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[0, 2, 4, 7, 3, 1, 9, 8, 6, 5]\n"}],"source":["import random\n","index = random.sample(range(0,10),10)\n","print(index)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["torch.save(cnn.state_dict(), '\\\\net\\\\net_parameters.pkl')"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[('conv1.weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('conv1.bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True)), ('conv2.weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('conv2.bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True)), ('fc1.weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('fc1.bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n"}],"source":["for m in cnn.modules():\n","    params = list(m.named_parameters())\n","    print(params)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}