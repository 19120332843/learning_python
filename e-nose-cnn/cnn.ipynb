{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"6\nEpoch[105], loss: 0.26610588\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9122448979591836, sum:447\nEpoch[106], loss: 0.26391968\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9122448979591836, sum:447\nEpoch[107], loss: 0.26199189\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9142857142857143, sum:448\nEpoch[108], loss: 0.26007032\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9142857142857143, sum:448\nEpoch[109], loss: 0.25768302\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9183673469387755, sum:450\nEpoch[110], loss: 0.25543624\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[111], loss: 0.25336023\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[112], loss: 0.25127383\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[113], loss: 0.24920283\ntotal:210, accuracy:0.7619047619047619, sum:160\n=============================================================================\ntotal_train:490, accuracy:0.9224489795918367, sum:452\nEpoch[114], loss: 0.24723812\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9224489795918367, sum:452\nEpoch[115], loss: 0.24540662\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9224489795918367, sum:452\nEpoch[116], loss: 0.24333882\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[117], loss: 0.24126619\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[118], loss: 0.23907545\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[119], loss: 0.23707054\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[120], loss: 0.23500161\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[121], loss: 0.23293015\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[122], loss: 0.23081689\ntotal:210, accuracy:0.7571428571428571, sum:159\n=============================================================================\ntotal_train:490, accuracy:0.926530612244898, sum:454\nEpoch[123], loss: 0.22870686\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9306122448979591, sum:456\nEpoch[124], loss: 0.22645709\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9306122448979591, sum:456\nEpoch[125], loss: 0.22437948\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9326530612244898, sum:457\nEpoch[126], loss: 0.22244446\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9326530612244898, sum:457\nEpoch[127], loss: 0.22033206\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[128], loss: 0.21809615\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[129], loss: 0.21617355\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[130], loss: 0.21417090\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[131], loss: 0.21187438\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[132], loss: 0.20972194\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[133], loss: 0.20768891\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[134], loss: 0.20527881\ntotal:210, accuracy:0.7523809523809524, sum:158\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[135], loss: 0.20304366\ntotal:210, accuracy:0.7523809523809524, sum:158\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[136], loss: 0.20109047\ntotal:210, accuracy:0.7523809523809524, sum:158\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[137], loss: 0.19899599\ntotal:210, accuracy:0.7523809523809524, sum:158\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[138], loss: 0.19686896\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[139], loss: 0.19484521\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[140], loss: 0.19304394\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[141], loss: 0.19097277\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[142], loss: 0.18889672\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[143], loss: 0.18724728\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[144], loss: 0.18552904\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[145], loss: 0.18347194\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[146], loss: 0.18171343\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[147], loss: 0.17988877\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[148], loss: 0.17798223\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[149], loss: 0.17656043\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[150], loss: 0.17496151\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[151], loss: 0.17328203\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[152], loss: 0.17144118\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[153], loss: 0.16981460\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[154], loss: 0.16847416\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9469387755102041, sum:464\nEpoch[155], loss: 0.16714497\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9469387755102041, sum:464\nEpoch[156], loss: 0.16569558\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9469387755102041, sum:464\nEpoch[157], loss: 0.16423262\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9469387755102041, sum:464\nEpoch[158], loss: 0.16310919\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9469387755102041, sum:464\nEpoch[159], loss: 0.16177912\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9469387755102041, sum:464\nEpoch[160], loss: 0.16041099\ntotal:210, accuracy:0.7428571428571429, sum:156\n=============================================================================\ntotal_train:490, accuracy:0.9448979591836735, sum:463\nEpoch[161], loss: 0.15935530\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9489795918367347, sum:465\nEpoch[162], loss: 0.15809857\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9489795918367347, sum:465\nEpoch[163], loss: 0.15678969\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9489795918367347, sum:465\nEpoch[164], loss: 0.15602349\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9448979591836735, sum:463\nEpoch[165], loss: 0.15501918\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9408163265306122, sum:461\nEpoch[166], loss: 0.15412168\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9408163265306122, sum:461\nEpoch[167], loss: 0.15293809\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9408163265306122, sum:461\nEpoch[168], loss: 0.15231553\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9408163265306122, sum:461\nEpoch[169], loss: 0.15154544\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9408163265306122, sum:461\nEpoch[170], loss: 0.15071278\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[171], loss: 0.15027697\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[172], loss: 0.14971684\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[173], loss: 0.14911386\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[174], loss: 0.14884937\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[175], loss: 0.14840234\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.936734693877551, sum:459\nEpoch[176], loss: 0.14764223\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9306122448979591, sum:456\nEpoch[177], loss: 0.14748026\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9285714285714286, sum:455\nEpoch[178], loss: 0.14751654\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.926530612244898, sum:454\nEpoch[179], loss: 0.14744323\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.926530612244898, sum:454\nEpoch[180], loss: 0.14808791\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[181], loss: 0.14806081\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[182], loss: 0.14848089\ntotal:210, accuracy:0.7238095238095238, sum:152\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[183], loss: 0.14907529\ntotal:210, accuracy:0.7238095238095238, sum:152\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[184], loss: 0.15016098\ntotal:210, accuracy:0.7238095238095238, sum:152\n=============================================================================\ntotal_train:490, accuracy:0.9224489795918367, sum:452\nEpoch[185], loss: 0.15116537\ntotal:210, accuracy:0.7238095238095238, sum:152\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[186], loss: 0.15220753\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9224489795918367, sum:452\nEpoch[187], loss: 0.15389094\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9204081632653062, sum:451\nEpoch[188], loss: 0.15460494\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9224489795918367, sum:452\nEpoch[189], loss: 0.15542616\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9244897959183673, sum:453\nEpoch[190], loss: 0.15636954\ntotal:210, accuracy:0.7238095238095238, sum:152\n=============================================================================\ntotal_train:490, accuracy:0.9326530612244898, sum:457\nEpoch[191], loss: 0.15675498\ntotal:210, accuracy:0.7238095238095238, sum:152\n=============================================================================\ntotal_train:490, accuracy:0.9326530612244898, sum:457\nEpoch[192], loss: 0.15648518\ntotal:210, accuracy:0.719047619047619, sum:151\n=============================================================================\ntotal_train:490, accuracy:0.9346938775510204, sum:458\nEpoch[193], loss: 0.15570438\ntotal:210, accuracy:0.7238095238095238, sum:152\n=============================================================================\ntotal_train:490, accuracy:0.9387755102040817, sum:460\nEpoch[194], loss: 0.15477891\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9408163265306122, sum:461\nEpoch[195], loss: 0.15356477\ntotal:210, accuracy:0.7285714285714285, sum:153\n=============================================================================\ntotal_train:490, accuracy:0.9428571428571428, sum:462\nEpoch[196], loss: 0.15220933\ntotal:210, accuracy:0.7333333333333333, sum:154\n=============================================================================\ntotal_train:490, accuracy:0.9510204081632653, sum:466\nEpoch[197], loss: 0.15183081\ntotal:210, accuracy:0.7380952380952381, sum:155\n=============================================================================\ntotal_train:490, accuracy:0.9612244897959183, sum:471\nEpoch[198], loss: 0.15224587\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\ntotal_train:490, accuracy:0.9612244897959183, sum:471\nEpoch[199], loss: 0.15271060\ntotal:210, accuracy:0.7523809523809524, sum:158\n=============================================================================\ntotal_train:490, accuracy:0.9673469387755103, sum:474\nEpoch[200], loss: 0.15258098\ntotal:210, accuracy:0.7476190476190476, sum:157\n=============================================================================\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import os \n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","\n","def Data_Reading(Normalization=True):\n","    trainset = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\trainset.npy')\n","    trainlabel = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\trainlabel.npy')\n","    testset = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\testset.npy')\n","    testlabel = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\testlabel.npy')\n","\n","    # Normalization\n","    train_Normed = Normlize(trainset)\n","    test_Normed = Normlize(testset)\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(train_Normed).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(trainlabel).type(torch.int64)\n","        test_x = torch.from_numpy(test_Normed).type(torch.cuda.FloatTensor)\n","        test_y = torch.from_numpy(testlabel).type(torch.int64)\n","    else:\n","        data = torch.from_numpy(data).type(torch.FloatTensor)\n","        label = torch.from_numpy(label).type(torch.int64)\n","\n","    # reshape\n","    train_x = train_x.view(490, 1, 10, 120)\n","    test_x = test_x.view(210, 1, 10, 120)\n","    \n","    # data = data.cpu().numpy()\n","    # label = label.numpy()    \n","    # train_x, test_x, train_y, test_y = train_test_split(data, label, test_size=0.3) \n","    # train_x = torch.from_numpy(train_x).type(torch.cuda.FloatTensor)\n","    # test_x = torch.from_numpy(test_x).type(torch.cuda.FloatTensor)\n","    # train_y = torch.from_numpy(train_y).type(torch.int64)\n","    # test_y = torch.from_numpy(test_y).type(torch.int64)\n","    \n","\n","    permutation = np.random.permutation(train_y.shape[0])\n","    train_x = train_x[permutation, :, :, :]\n","    print('---------------------------------------------------------------------------')\n","    train_y = train_y[permutation]\n","    return train_x, test_x, train_y, test_y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1,6,(10,4),stride=(1,2))#（120 - 4）/2 + 1 = 59\n","        self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,1))#57  \n","        self.fc1 = nn.Linear(10*1*57,7)\n","       \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.001)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        return x#\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n","\n","    print(device)\n","\n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    optimizer = optim.SGD(cnn.parameters(), lr=0.01, momentum=0.8)#\n","    loss_func = nn.CrossEntropyLoss()#CrossEntropyLoss()\n","\n","    train_x, test_x, train_y, test_y = Data_Reading(Normalization=1)\n","    train_y = train_y.squeeze()\n","    test_y = test_y.squeeze()\n","    train_x = train_x.to(device)\n","    test_x = test_x.to(device)\n","    train_y = train_y.to(device)\n","    test_y = test_y.to(device)\n","\n","    \n","\n","    #train\n","    sum = 0\n","\n","    batch_size = 49\n","    tr_x = Variable(train_x)\n","    tr_y = Variable(train_y)\n","    for epoch in range(200):\n","        running_loss = 0.0\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            out = cnn(t_x)\n","            loss = loss_func(out, t_y)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","   \n","        running_loss = running_loss / 10\n","\n","        out = cnn(tr_x)\n","        predicted_train = torch.max(out.data, 1)[1]\n","        total_train = tr_y.size(0)\n","        for j in range(tr_y.size(0)):\n","            if predicted_train[j] == tr_y[j]:\n","                sum += 1\n","        \n","        print('total_train:{}, accuracy:{}, sum:{}'.format(total_train, sum / total_train, sum))\n","        sum = 0\n","\n","        if (sum / total_train > 0.85) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.4)\n","        elif (sum / total_train > 0.95) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=0.0001)#momentum=0\n","\n","\n","        print('Epoch[{}], loss: {:.8f}'.format(epoch + 1, running_loss))\n","    \n","    \n","    #test\n","        te_x = Variable(test_x)\n","        te_y = Variable(test_y)\n","        out1 = cnn(te_x)\n","        predicted_test = torch.max(out1.data, 1)[1]#.data.squeeze()\n","        total = te_y.size(0)\n","\n","        for j in range(te_y.size(0)):\n","            if predicted_test[j] == te_y[j]:\n","                sum += 1\n","        print('total:{}, accuracy:{}, sum:{}'.format(total, sum / total, sum))\n","        print('=============================================================================')\n","        sum = 0"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[0, 2, 4, 7, 3, 1, 9, 8, 6, 5]\n"}],"source":["import random\n","index = random.sample(range(0,10),10)\n","print(index)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["torch.save(cnn.state_dict(), '\\\\net\\\\net_parameters.pkl')"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[('conv1.weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('conv1.bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True)), ('conv2.weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('conv2.bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True)), ('fc1.weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('fc1.bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n"}],"source":["for m in cnn.modules():\n","    params = list(m.named_parameters())\n","    print(params)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}