{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"cpu\nNet(\n  (conv1): Conv2d(1, 6, kernel_size=(4, 4), stride=(2, 2))\n  (conv2): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 2))\n  (conv3): Conv2d(10, 14, kernel_size=(2, 2), stride=(1, 1))\n  (fc): Linear(in_features=392, out_features=4, bias=True)\n)\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\nEpoch[1], loss: 1.386628334721, correct:134\nEpoch[2], loss: 1.383821229140, correct:125\nEpoch[3], loss: 1.381499320269, correct:124\nEpoch[4], loss: 1.379791364074, correct:120\nEpoch[5], loss: 1.378396153450, correct:121\nEpoch[6], loss: 1.376619661848, correct:121\nEpoch[7], loss: 1.373700072368, correct:121\nEpoch[8], loss: 1.370168628792, correct:121\nEpoch[9], loss: 1.368113567432, correct:118\nEpoch[10], loss: 1.367195203900, correct:119\nEpoch[11], loss: 1.366660582523, correct:118\nEpoch[12], loss: 1.366253472865, correct:120\nEpoch[13], loss: 1.365862491230, correct:120\nEpoch[14], loss: 1.365406451126, correct:124\nEpoch[15], loss: 1.364768753449, correct:120\nEpoch[16], loss: 1.363704748452, correct:147\nEpoch[17], loss: 1.361620540420, correct:147\nEpoch[18], loss: 1.357604722182, correct:139\nEpoch[19], loss: 1.354333728552, correct:135\nEpoch[20], loss: 1.353085796038, correct:135\nEpoch[21], loss: 1.352329522371, correct:135\nEpoch[22], loss: 1.351773368816, correct:134\nEpoch[23], loss: 1.351349622011, correct:134\nEpoch[24], loss: 1.350930149357, correct:136\nEpoch[25], loss: 1.350690948466, correct:130\nEpoch[26], loss: 1.350323289633, correct:128\nEpoch[27], loss: 1.351478022834, correct:119\nEpoch[28], loss: 1.354182752470, correct:148\nEpoch[29], loss: 1.374420344830, correct:118\nEpoch[30], loss: 1.351774906119, correct:116\nEpoch[31], loss: 1.353144404789, correct:146\nEpoch[32], loss: 1.366390796999, correct:115\nEpoch[33], loss: 1.349643116196, correct:121\nEpoch[34], loss: 1.349199175835, correct:121\nEpoch[35], loss: 1.348723858595, correct:124\nEpoch[36], loss: 1.348527962963, correct:122\nEpoch[37], loss: 1.348153067132, correct:132\nEpoch[38], loss: 1.352128292123, correct:111\nEpoch[39], loss: 1.358692372839, correct:150\nEpoch[40], loss: 1.395136843125, correct:109\nEpoch[41], loss: 1.370260626078, correct:110\nEpoch[42], loss: 1.350600565473, correct:113\nEpoch[43], loss: 1.350006093582, correct:142\nEpoch[44], loss: 1.356162960331, correct:110\nEpoch[45], loss: 1.349216672281, correct:143\nEpoch[46], loss: 1.355875693262, correct:110\nEpoch[47], loss: 1.348957821727, correct:146\nEpoch[48], loss: 1.357840761542, correct:109\nEpoch[49], loss: 1.347347927590, correct:145\nEpoch[50], loss: 1.353461210926, correct:109\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import os \n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","\n","def Data_Reading(Normalization=True):\n","    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainset.npy')\n","    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainlabel.npy')\n","\n","    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testset.npy')\n","    test_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testlabel.npy')\n","\n","    # Normalization\n","    train_x_Normed = Normlize(train_x)\n","\n","    test_x_Normed = Normlize(test_x)\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n","\n","        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n","\n","        train_y = torch.from_numpy(train_y).type(torch.cuda.int64)\n","\n","        test_y = torch.from_numpy(test_y).type(torch.cuda.int64)\n","\n","    else:\n","        train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n","\n","        test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n","\n","        train_y = torch.from_numpy(train_y).type(torch.int64)\n","\n","        test_y = torch.from_numpy(test_y).type(torch.int64)\n","    # reshape\n","    train_x = train_x.view(408, 1, 10, 120)\n","    test_x = test_x.view(202, 1, 10, 120)\n","\n","    return train_x, test_x, train_y, test_y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1,6,(4,4),stride=(2,2))#(10 - 4)/2 + 1 = 4   (120 - 4)/2 + 1 = 59 \n","        self.conv2 = nn.Conv2d(6,10,(3,3),stride=(1,2))#(4 - 3)/1 + 1 = 2   (59 - 3)/2 + 1 = 29 \n","        self.conv3 = nn.Conv2d(10,14,(2,2),stride=(1,1))#(2 - 2)/1 + 1 = 1  (29 - 2)/1 + 1 = 28\n","        self.classes = [0, 1, 2, 3]\n","        # self.conv1 = nn.Conv2d(1,6,(10,3),stride=(1,3))\n","        # self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,2))\n","        self.fc = nn.Linear(14*1*28,4)\n","        \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.01)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1) # flatten the tensor\n","        x = self.fc(x)\n","        # return x\n","        return F.log_softmax(x,dim=0)\n","\n","\n","#training\n","batch_size = 17\n","def train(train_x,train_y,step=10):\n","    for epoch in range(50):\n","        running_loss = 0.0\n","        total = 0.0\n","        correct = 0.0\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            out = cnn(t_x)\n","            #forward\n","            #loss_func = nn.MSELoss() 均方损失函数  loss(x(i),y(i)) = (x(i) - y(i))^2\n","            loss = loss_func(out, t_y)#.long().squeeze()\n","            #梯度初始化为零\n","            optimizer.zero_grad()\n","             \n","            #backward\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            _, predicted = torch.max(out.data, 1)\n","            # print(predicted)\n","            total += t_y.size(0)\n","            # print(total)\n","            correct += predicted.eq(t_y.data).cpu().sum()\n","            \n","        running_loss = running_loss / 24\n","\n","        # if (epoch + 1) % step == 0:\n","        print('Epoch[{}], loss: {:.12f}, correct:{}'.format(epoch + 1, running_loss, correct))\n","        \n","\n","def predict(test_x,test_y):\n","    test_acc = 0.0\n","\n","    te_x = Variable(test_x)\n","    tey1 = Variable(test_y)\n","    out1 = cnn(te_x)\n","    loss = loss_func(out1, tey1)\n","\n","    _, predicted = torch.max(out1.data, 1)\n","    print(predict)\n","\n","    print('loss:{}, out:{}, tabel:{}'.format(loss, out1, tey1))\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n","\n","    print(device)\n","\n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.8)\n","    # loss_func = nn.MSELoss()\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    train_x, test_x, train_y, test_y = Data_Reading(Normalization=0)\n","    train_y = train_y.squeeze()\n","    test_y = test_y.squeeze()\n","    train_x = train_x.to(device)\n","    test_x = test_x.to(device)\n","    train_y = train_y.to(device)\n","    test_y = test_y.to(device)\n","    print(train_y)\n","\n","    train(train_x, train_y, step=10)\n","    # predict(test_x,test_y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}