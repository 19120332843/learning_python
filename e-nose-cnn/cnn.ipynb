{"cells":[{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"cuda:0\nNet(\n  (conv1): Conv2d(1, 6, kernel_size=(10, 3), stride=(1, 3))\n  (conv2): Conv2d(6, 10, kernel_size=(1, 4), stride=(1, 2))\n  (fc): Linear(in_features=190, out_features=4, bias=True)\n)\nEpoch[10], loss: 5.24723669\nEpoch[20], loss: 5.24723323\nEpoch[30], loss: 5.24722970\nEpoch[40], loss: 5.24722606\nEpoch[50], loss: 5.24722282\nEpoch[60], loss: 5.24721968\nEpoch[70], loss: 5.24721658\nEpoch[80], loss: 5.24721368\nEpoch[90], loss: 5.24721054\nEpoch[100], loss: 5.24720750\nEpoch[110], loss: 5.24720448\nEpoch[120], loss: 5.24720122\nEpoch[130], loss: 5.24719799\nEpoch[140], loss: 5.24719477\nEpoch[150], loss: 5.24719153\nEpoch[160], loss: 5.24718833\nEpoch[170], loss: 5.24718503\nEpoch[180], loss: 5.24718171\nEpoch[190], loss: 5.24717842\nEpoch[200], loss: 5.24717478\nout:tensor([189,  18, 152, 152, 189,  18, 189, 189, 152,  18,  18,  18,  18,  18,\n        152, 189, 189,  18, 188,  18, 152, 152, 152, 183, 152,  18, 152,  18,\n        185, 185, 187, 152, 152, 152, 152, 152, 152, 152, 152,   6, 177, 152,\n         58, 152,  58, 152, 152, 152,   3, 152, 152,  61,  61,  64, 120, 169,\n        168,  62, 172, 169,  64, 169,  18,  63,  63,  62,  63,  62,  63,  63,\n        151,  61,  62,  18,  18,  62,  62,  18,  63,  18,  18,  62,  62,  18,\n         61,  60,  60, 152,  61, 170,  60, 172,  63,  64, 121, 122, 118,  64,\n        152, 152,  62,  63,  62,  18,   3,   3, 174, 152, 152, 152, 152, 152,\n          5,  13,  20,   7, 174,   7, 185, 189,   7,   7, 189,   3,   3,   3,\n        174, 174, 156, 189, 156, 185,  23,  37,  25,  37,  37,  37,  37,  37,\n         37,  37,  37,  37, 185, 176,  57,  37,  57,  57,  58,  37,  57,  57,\n         57, 173,  57,  57,  57,  57,  57,  57, 175,  57,  57,  57,  57,  57,\n         57,  57,   0,  57,  57,  57,  57,  57,  57,  57,  57, 133,  57,  57,\n         57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,  57,\n         57,  57,  57,  57,  57,  57], device='cuda:0'), total:202, accuracy:0.0, sum:0\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import os \n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","\n","def Data_Reading(Normalization=True):\n","    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainset.npy')\n","    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainlabel.npy')\n","\n","    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testset.npy')\n","    test_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testlabel.npy')\n","\n","    # Normalization\n","    train_x_Normed = Normlize(train_x)\n","\n","    test_x_Normed = Normlize(test_x)\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n","\n","        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n","\n","        train_y = torch.from_numpy(train_y).type(torch.int64)\n","\n","        test_y = torch.from_numpy(test_y).type(torch.int64)\n","\n","    else:\n","        train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n","\n","        test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n","\n","        train_y = torch.from_numpy(train_y).type(torch.int64)\n","\n","        test_y = torch.from_numpy(test_y).type(torch.int64)\n","    # reshape\n","    train_x = train_x.view(408, 1, 10, 120)\n","    test_x = test_x.view(202, 1, 10, 120)\n","\n","    return train_x, test_x, train_y, test_y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # self.conv1 = nn.Conv2d(1,6,(4,4),stride=(2,2))#(10 - 4)/2 + 1 = 4   (120 - 4)/2 + 1 = 59 \n","        # self.conv2 = nn.Conv2d(6,10,(3,3),stride=(1,2))#(4 - 3)/1 + 1 = 2   (59 - 3)/2 + 1 = 29 \n","        # self.conv3 = nn.Conv2d(10,14,(2,2),stride=(1,1))#(2 - 2)/1 + 1 = 1  (29 - 2)/1 + 1 = 28\n","        self.conv1 = nn.Conv2d(1,6,(10,3),stride=(1,3))#40\n","        self.conv2 = nn.Conv2d(6,10,(1,4),stride=(1,2))#19\n","        self.fc = nn.Linear(10*1*19,4)\n","        \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.01)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        # x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        # return x\n","        return F.log_softmax(x,dim=0)\n","\n","\n","#training\n","batch_size = 17\n","def train(train_x,train_y,step=10):\n","    for epoch in range(200):\n","        running_loss = 0.0\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            out = cnn(t_x)\n","            #forward\n","            loss = loss_func(out, t_y)\n","            #梯度初始化为零\n","            optimizer.zero_grad()\n","             \n","            #backward\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","   \n","        running_loss = running_loss / 24\n","\n","        if (epoch + 1) % step == 0:\n","            print('Epoch[{}], loss: {:.8f}'.format(epoch + 1, running_loss))\n","        \n","\n","def classification(test_x,test_y):\n","    sum = 0\n","\n","    te_x = Variable(test_x)\n","    tey1 = Variable(test_y)\n","    out1 = cnn(te_x)\n","    predicted1 = torch.max(out1.data, 1)[1].data.squeeze()\n","    total = tey1.size(0)\n","\n","    for j in range(tey1.size(0)):\n","        if predicted1[j] == tey1[j]:\n","            sum += 1\n","\n","    print('out:{}, total:{}, accuracy:{}, sum:{}'.format(predicted1, total, sum / total, sum))\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n","\n","    print(device)\n","\n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.8)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    train_x, test_x, train_y, test_y = Data_Reading(Normalization=1)\n","    train_y = train_y.squeeze()\n","    test_y = test_y.squeeze()\n","    train_x = train_x.to(device)\n","    test_x = test_x.to(device)\n","    train_y = train_y.to(device)\n","    test_y = test_y.to(device)\n","\n","    train(train_x, train_y, step=10)\n","    classification(test_x,test_y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}