{"cells":[{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"cuda:0\nNet(\n  (conv1): Conv2d(1, 3, kernel_size=(1, 3), stride=(1, 1))\n  (conv2): Conv2d(3, 6, kernel_size=(2, 3), stride=(2, 1))\n  (conv3): Conv2d(6, 10, kernel_size=(5, 3), stride=(1, 1))\n  (fc1): Linear(in_features=550, out_features=4, bias=True)\n)\ntotal:174, accuracy:0.3045977011494253, sum:53\ntotal_train:410, accuracy:0.25853658536585367, sum:106\nEpoch[2], loss: 1.38788423\ntotal:174, accuracy:0.3850574712643678, sum:67\ntotal:174, accuracy:0.45977011494252873, sum:80\ntotal_train:410, accuracy:0.35853658536585364, sum:147\nEpoch[4], loss: 1.38563962\ntotal:174, accuracy:0.47126436781609193, sum:82\ntotal:174, accuracy:0.47701149425287354, sum:83\ntotal_train:410, accuracy:0.3878048780487805, sum:159\nEpoch[6], loss: 1.38348288\ntotal:174, accuracy:0.47126436781609193, sum:82\ntotal:174, accuracy:0.47126436781609193, sum:82\ntotal_train:410, accuracy:0.3878048780487805, sum:159\nEpoch[8], loss: 1.38134357\ntotal:174, accuracy:0.47126436781609193, sum:82\ntotal:174, accuracy:0.47701149425287354, sum:83\ntotal_train:410, accuracy:0.4097560975609756, sum:168\nEpoch[10], loss: 1.37920193\ntotal:174, accuracy:0.4827586206896552, sum:84\ntotal:174, accuracy:0.4827586206896552, sum:84\ntotal_train:410, accuracy:0.43902439024390244, sum:180\nEpoch[12], loss: 1.37702508\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.47073170731707314, sum:193\nEpoch[14], loss: 1.37479211\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.5024390243902439, sum:206\nEpoch[16], loss: 1.37249213\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.5317073170731708, sum:218\nEpoch[18], loss: 1.37010179\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.5, sum:87\ntotal_train:410, accuracy:0.5560975609756098, sum:228\nEpoch[20], loss: 1.36760403\ntotal:174, accuracy:0.5, sum:87\ntotal:174, accuracy:0.5, sum:87\ntotal_train:410, accuracy:0.573170731707317, sum:235\nEpoch[22], loss: 1.36497040\ntotal:174, accuracy:0.5, sum:87\ntotal:174, accuracy:0.5, sum:87\ntotal_train:410, accuracy:0.5951219512195122, sum:244\nEpoch[24], loss: 1.36218530\ntotal:174, accuracy:0.5, sum:87\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6024390243902439, sum:247\nEpoch[26], loss: 1.35923380\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6097560975609756, sum:250\nEpoch[28], loss: 1.35607141\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6146341463414634, sum:252\nEpoch[30], loss: 1.35269705\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6170731707317073, sum:253\nEpoch[32], loss: 1.34906277\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6121951219512195, sum:251\nEpoch[34], loss: 1.34515862\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6121951219512195, sum:251\nEpoch[36], loss: 1.34098101\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6170731707317073, sum:253\nEpoch[38], loss: 1.33646090\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6170731707317073, sum:253\nEpoch[40], loss: 1.33158237\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.6195121951219512, sum:254\nEpoch[42], loss: 1.32630286\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.624390243902439, sum:256\nEpoch[44], loss: 1.32051358\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.6317073170731707, sum:259\nEpoch[46], loss: 1.31421019\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.6292682926829268, sum:258\nEpoch[48], loss: 1.30729522\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.6292682926829268, sum:258\nEpoch[50], loss: 1.29968483\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.6268292682926829, sum:257\nEpoch[52], loss: 1.29134558\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.6292682926829268, sum:258\nEpoch[54], loss: 1.28211650\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal_train:410, accuracy:0.6341463414634146, sum:260\nEpoch[56], loss: 1.27190010\ntotal:174, accuracy:0.4885057471264368, sum:85\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6414634146341464, sum:263\nEpoch[58], loss: 1.26056387\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal:174, accuracy:0.4942528735632184, sum:86\ntotal_train:410, accuracy:0.6560975609756098, sum:269\nEpoch[60], loss: 1.24791448\ntotal:174, accuracy:0.5, sum:87\ntotal:174, accuracy:0.5, sum:87\ntotal_train:410, accuracy:0.6878048780487804, sum:282\nEpoch[62], loss: 1.23376257\ntotal:174, accuracy:0.5057471264367817, sum:88\ntotal:174, accuracy:0.5057471264367817, sum:88\ntotal_train:410, accuracy:0.7219512195121951, sum:296\nEpoch[64], loss: 1.21785141\ntotal:174, accuracy:0.5229885057471264, sum:91\ntotal:174, accuracy:0.5287356321839081, sum:92\ntotal_train:410, accuracy:0.7731707317073171, sum:317\nEpoch[66], loss: 1.19990603\ntotal:174, accuracy:0.5459770114942529, sum:95\ntotal:174, accuracy:0.5574712643678161, sum:97\ntotal_train:410, accuracy:0.802439024390244, sum:329\nEpoch[68], loss: 1.17954686\ntotal:174, accuracy:0.5574712643678161, sum:97\ntotal:174, accuracy:0.5574712643678161, sum:97\ntotal_train:410, accuracy:0.8170731707317073, sum:335\nEpoch[70], loss: 1.15640262\ntotal:174, accuracy:0.5574712643678161, sum:97\ntotal:174, accuracy:0.5517241379310345, sum:96\ntotal_train:410, accuracy:0.8268292682926829, sum:339\nEpoch[72], loss: 1.13006690\ntotal:174, accuracy:0.5517241379310345, sum:96\ntotal:174, accuracy:0.5517241379310345, sum:96\ntotal_train:410, accuracy:0.8463414634146341, sum:347\nEpoch[74], loss: 1.09997729\ntotal:174, accuracy:0.5574712643678161, sum:97\ntotal:174, accuracy:0.5632183908045977, sum:98\ntotal_train:410, accuracy:0.8585365853658536, sum:352\nEpoch[76], loss: 1.06566224\ntotal:174, accuracy:0.5747126436781609, sum:100\ntotal:174, accuracy:0.5804597701149425, sum:101\ntotal_train:410, accuracy:0.8560975609756097, sum:351\nEpoch[78], loss: 1.02667060\ntotal:174, accuracy:0.5919540229885057, sum:103\ntotal:174, accuracy:0.603448275862069, sum:105\ntotal_train:410, accuracy:0.8634146341463415, sum:354\nEpoch[80], loss: 0.98272082\ntotal:174, accuracy:0.6091954022988506, sum:106\ntotal:174, accuracy:0.6149425287356322, sum:107\ntotal_train:410, accuracy:0.8731707317073171, sum:358\nEpoch[82], loss: 0.93374545\ntotal:174, accuracy:0.6206896551724138, sum:108\ntotal:174, accuracy:0.6206896551724138, sum:108\ntotal_train:410, accuracy:0.8682926829268293, sum:356\nEpoch[84], loss: 0.88003699\ntotal:174, accuracy:0.6206896551724138, sum:108\ntotal:174, accuracy:0.6206896551724138, sum:108\ntotal_train:410, accuracy:0.8609756097560975, sum:353\nEpoch[86], loss: 0.82258818\ntotal:174, accuracy:0.6264367816091954, sum:109\ntotal:174, accuracy:0.632183908045977, sum:110\ntotal_train:410, accuracy:0.8560975609756097, sum:351\nEpoch[88], loss: 0.76344409\ntotal:174, accuracy:0.632183908045977, sum:110\ntotal:174, accuracy:0.632183908045977, sum:110\ntotal_train:410, accuracy:0.8560975609756097, sum:351\nEpoch[90], loss: 0.70461863\ntotal:174, accuracy:0.632183908045977, sum:110\ntotal:174, accuracy:0.632183908045977, sum:110\ntotal_train:410, accuracy:0.8585365853658536, sum:352\nEpoch[92], loss: 0.64799528\ntotal:174, accuracy:0.632183908045977, sum:110\ntotal:174, accuracy:0.632183908045977, sum:110\ntotal_train:410, accuracy:0.8609756097560975, sum:353\nEpoch[94], loss: 0.59473671\ntotal:174, accuracy:0.6379310344827587, sum:111\ntotal:174, accuracy:0.6379310344827587, sum:111\ntotal_train:410, accuracy:0.8658536585365854, sum:355\nEpoch[96], loss: 0.54571413\ntotal:174, accuracy:0.6436781609195402, sum:112\ntotal:174, accuracy:0.6551724137931034, sum:114\ntotal_train:410, accuracy:0.8707317073170732, sum:357\nEpoch[98], loss: 0.50131010\ntotal:174, accuracy:0.6666666666666666, sum:116\ntotal:174, accuracy:0.6666666666666666, sum:116\ntotal_train:410, accuracy:0.875609756097561, sum:359\nEpoch[100], loss: 0.46137902\ntotal:174, accuracy:0.6666666666666666, sum:116\ntotal:174, accuracy:0.6666666666666666, sum:116\ntotal_train:410, accuracy:0.8780487804878049, sum:360\nEpoch[102], loss: 0.42552655\ntotal:174, accuracy:0.6724137931034483, sum:117\ntotal:174, accuracy:0.6724137931034483, sum:117\ntotal_train:410, accuracy:0.8902439024390244, sum:365\nEpoch[104], loss: 0.39350083\ntotal:174, accuracy:0.6724137931034483, sum:117\ntotal:174, accuracy:0.6724137931034483, sum:117\ntotal_train:410, accuracy:0.8926829268292683, sum:366\nEpoch[106], loss: 0.36462805\ntotal:174, accuracy:0.6781609195402298, sum:118\ntotal:174, accuracy:0.6781609195402298, sum:118\ntotal_train:410, accuracy:0.9, sum:369\nEpoch[108], loss: 0.33849048\ntotal:174, accuracy:0.6781609195402298, sum:118\ntotal:174, accuracy:0.6781609195402298, sum:118\ntotal_train:410, accuracy:0.9024390243902439, sum:370\nEpoch[110], loss: 0.31493168\ntotal:174, accuracy:0.6954022988505747, sum:121\ntotal:174, accuracy:0.6954022988505747, sum:121\ntotal_train:410, accuracy:0.9121951219512195, sum:374\nEpoch[112], loss: 0.29358114\ntotal:174, accuracy:0.7068965517241379, sum:123\ntotal:174, accuracy:0.7126436781609196, sum:124\ntotal_train:410, accuracy:0.9195121951219513, sum:377\nEpoch[114], loss: 0.27410973\ntotal:174, accuracy:0.7126436781609196, sum:124\ntotal:174, accuracy:0.7183908045977011, sum:125\ntotal_train:410, accuracy:0.9219512195121952, sum:378\nEpoch[116], loss: 0.25640706\ntotal:174, accuracy:0.7241379310344828, sum:126\ntotal:174, accuracy:0.735632183908046, sum:128\ntotal_train:410, accuracy:0.9317073170731708, sum:382\nEpoch[118], loss: 0.24017159\ntotal:174, accuracy:0.7413793103448276, sum:129\ntotal:174, accuracy:0.7528735632183908, sum:131\ntotal_train:410, accuracy:0.9390243902439024, sum:385\nEpoch[120], loss: 0.22522677\ntotal:174, accuracy:0.7528735632183908, sum:131\ntotal:174, accuracy:0.7528735632183908, sum:131\ntotal_train:410, accuracy:0.948780487804878, sum:389\nEpoch[122], loss: 0.21154646\ntotal:174, accuracy:0.764367816091954, sum:133\ntotal:174, accuracy:0.7758620689655172, sum:135\ntotal_train:410, accuracy:0.9634146341463414, sum:395\nEpoch[124], loss: 0.19900896\ntotal:174, accuracy:0.7758620689655172, sum:135\ntotal:174, accuracy:0.7873563218390804, sum:137\ntotal_train:410, accuracy:0.9658536585365853, sum:396\nEpoch[126], loss: 0.18752584\ntotal:174, accuracy:0.7931034482758621, sum:138\ntotal:174, accuracy:0.8045977011494253, sum:140\ntotal_train:410, accuracy:0.9658536585365853, sum:396\nEpoch[128], loss: 0.17693916\ntotal:174, accuracy:0.8045977011494253, sum:140\ntotal:174, accuracy:0.8045977011494253, sum:140\ntotal_train:410, accuracy:0.9707317073170731, sum:398\nEpoch[130], loss: 0.16717892\ntotal:174, accuracy:0.8045977011494253, sum:140\ntotal:174, accuracy:0.8103448275862069, sum:141\ntotal_train:410, accuracy:0.973170731707317, sum:399\nEpoch[132], loss: 0.15813211\ntotal:174, accuracy:0.8103448275862069, sum:141\ntotal:174, accuracy:0.8103448275862069, sum:141\ntotal_train:410, accuracy:0.973170731707317, sum:399\nEpoch[134], loss: 0.14972788\ntotal:174, accuracy:0.8103448275862069, sum:141\ntotal:174, accuracy:0.8103448275862069, sum:141\ntotal_train:410, accuracy:0.975609756097561, sum:400\nEpoch[136], loss: 0.14193122\ntotal:174, accuracy:0.8103448275862069, sum:141\ntotal:174, accuracy:0.8160919540229885, sum:142\ntotal_train:410, accuracy:0.9804878048780488, sum:402\nEpoch[138], loss: 0.13471655\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9804878048780488, sum:402\nEpoch[140], loss: 0.12798882\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9829268292682927, sum:403\nEpoch[142], loss: 0.12169682\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9829268292682927, sum:403\nEpoch[144], loss: 0.11579610\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9829268292682927, sum:403\nEpoch[146], loss: 0.11026282\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9829268292682927, sum:403\nEpoch[148], loss: 0.10504642\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9829268292682927, sum:403\nEpoch[150], loss: 0.10015263\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9829268292682927, sum:403\nEpoch[152], loss: 0.09553732\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9829268292682927, sum:403\nEpoch[154], loss: 0.09117463\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9853658536585366, sum:404\nEpoch[156], loss: 0.08709440\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9853658536585366, sum:404\nEpoch[158], loss: 0.08328900\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal:174, accuracy:0.8218390804597702, sum:143\ntotal_train:410, accuracy:0.9853658536585366, sum:404\nEpoch[160], loss: 0.07968172\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal_train:410, accuracy:0.9853658536585366, sum:404\nEpoch[162], loss: 0.07629668\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal_train:410, accuracy:0.9853658536585366, sum:404\nEpoch[164], loss: 0.07311185\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal_train:410, accuracy:0.9853658536585366, sum:404\nEpoch[166], loss: 0.07012999\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal_train:410, accuracy:0.9853658536585366, sum:404\nEpoch[168], loss: 0.06731603\ntotal:174, accuracy:0.8275862068965517, sum:144\ntotal:174, accuracy:0.8333333333333334, sum:145\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[170], loss: 0.06468008\ntotal:174, accuracy:0.8333333333333334, sum:145\ntotal:174, accuracy:0.8333333333333334, sum:145\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[172], loss: 0.06219767\ntotal:174, accuracy:0.8333333333333334, sum:145\ntotal:174, accuracy:0.8333333333333334, sum:145\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[174], loss: 0.05986169\ntotal:174, accuracy:0.8333333333333334, sum:145\ntotal:174, accuracy:0.8390804597701149, sum:146\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[176], loss: 0.05765467\ntotal:174, accuracy:0.8448275862068966, sum:147\ntotal:174, accuracy:0.8448275862068966, sum:147\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[178], loss: 0.05555603\ntotal:174, accuracy:0.8448275862068966, sum:147\ntotal:174, accuracy:0.8448275862068966, sum:147\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[180], loss: 0.05358268\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[182], loss: 0.05171284\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[184], loss: 0.04993189\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal_train:410, accuracy:0.9902439024390244, sum:406\nEpoch[186], loss: 0.04824776\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal_train:410, accuracy:0.9926829268292683, sum:407\nEpoch[188], loss: 0.04663971\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal_train:410, accuracy:0.9926829268292683, sum:407\nEpoch[190], loss: 0.04510585\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal:174, accuracy:0.8563218390804598, sum:149\ntotal_train:410, accuracy:0.9926829268292683, sum:407\nEpoch[192], loss: 0.04363942\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal_train:410, accuracy:0.9926829268292683, sum:407\nEpoch[194], loss: 0.04223948\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal_train:410, accuracy:0.9926829268292683, sum:407\nEpoch[196], loss: 0.04090092\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal_train:410, accuracy:0.9926829268292683, sum:407\nEpoch[198], loss: 0.03962024\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal:174, accuracy:0.8505747126436781, sum:148\ntotal_train:410, accuracy:0.9926829268292683, sum:407\nEpoch[200], loss: 0.03840306\ntotal:174, accuracy:0.8505747126436781, sum:148\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import os \n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","\n","def Data_Reading(Normalization=True):\n","    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainset2.npy')\n","    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainlabel2.npy')\n","    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testset2.npy')\n","    test_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testlabel2.npy')\n","\n","    # Normalization\n","    train_x_Normed = Normlize(train_x)\n","    test_x_Normed = Normlize(test_x)\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n","        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.int64)\n","        test_y = torch.from_numpy(test_y).type(torch.int64)\n","\n","    else:\n","        train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n","        test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.int64)\n","        test_y = torch.from_numpy(test_y).type(torch.int64)\n","    # reshape\n","    train_x = train_x.view(410, 1, 10, 120)\n","    test_x = test_x.view(174, 1, 10, 120)\n","    # train_x = train_x.view(408, 1, 10, 120)\n","    # test_x = test_x.view(202, 1, 10, 120)\n","\n","    return train_x, test_x, train_y, test_y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1,3,(1,3),stride=(1,1))#(10 - 1)/1 + 1 = 10   (120 - 3)/1 + 1 = 118 118/2 = 59\n","        self.conv2 = nn.Conv2d(3,6,(2,3),stride=(2,1))#(10 - 2)/2 + 1 = 5   (59 - 3)/1 + 1 = 57 \n","        self.conv3 = nn.Conv2d(6,10,(5,3),stride=(1,1))# (5 - 5)/1 + 1 = 1  (57 - 3)/1 + 1 = 55\n","        self.fc1 = nn.Linear(10*1*55,4)\n","\n","       \n","        # self.conv1 = nn.Conv2d(1,6,(10,4),stride=(1,2))#59\n","        # self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,1))#57      \n","        # self.fc1 = nn.Linear(10*1*57,4)\n","\n","        # self.fc2 = nn.Linear(342,4)\n","        # self.fc3 = nn.Linear(85,4)\n","        # self.fc4 = nn.Linear(64,4)\n","        \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.01)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, (1,2))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        # x = F.logsigmoid(self.fc1(x))\n","        # x = F.logsigmoid(self.fc2(x))\n","        # x = F.relu(self.fc3(x))\n","        x = self.fc1(x)\n","        # return x\n","        return x#F.softmax(x,dim=0)\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n","\n","    print(device)\n","\n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.6)#\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    train_x, test_x, train_y, test_y = Data_Reading(Normalization=1)\n","    train_y = train_y.squeeze()\n","    test_y = test_y.squeeze()\n","    train_x = train_x.to(device)\n","    test_x = test_x.to(device)\n","    train_y = train_y.to(device)\n","    test_y = test_y.to(device)\n","\n","    \n","\n","    #train\n","    sum = 0\n","\n","    batch_size = 17\n","    step = 10\n","    tr_x = Variable(train_x)\n","    tr_y = Variable(train_y)\n","    for epoch in range(200):\n","        running_loss = 0.0\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            out = cnn(t_x)\n","            #forward\n","            loss = loss_func(out, t_y)\n","            #梯度初始化为零\n","            optimizer.zero_grad()\n","             \n","            #backward\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            # print('loss = {}, i = {}'.format(loss.item(), i))\n","\n","            # predicted1 = torch.max(out1.data, 1)[1]\n","   \n","        running_loss = running_loss / 24\n","\n","        out = cnn(tr_x)\n","        predicted_train = torch.max(out.data, 1)[1]\n","        total_train = tr_y.size(0)\n","        for j in range(tr_y.size(0)):\n","            if predicted_train[j] == tr_y[j]:\n","                sum += 1\n","        \n","        if (epoch + 1) % 2 == 0:\n","            print('total_train:{}, accuracy:{}, sum:{}'.format(total_train, sum / total_train, sum))\n","        sum = 0\n","\n","        if (sum / total_train > 0.85) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.4)\n","        elif (sum / total_train > 0.95) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=0.00001, momentum=0.2)\n","\n","        if (epoch + 1) % 2 == 0:\n","            print('Epoch[{}], loss: {:.8f}'.format(epoch + 1, running_loss))\n","    \n","    \n","    #test\n","        te_x = Variable(test_x)\n","        te_y = Variable(test_y)\n","        out1 = cnn(te_x)\n","        predicted_test = torch.max(out1.data, 1)[1]#.data.squeeze()\n","        total = te_y.size(0)\n","\n","        for j in range(te_y.size(0)):\n","            if predicted_test[j] == te_y[j]:\n","                sum += 1\n","\n","        print('total:{}, accuracy:{}, sum:{}'.format(total, sum / total, sum))\n","        sum = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[0, 2, 4, 7, 3, 1, 9, 8, 6, 5]\n"}],"source":["import random\n","index = random.sample(range(0,10),10)\n","print(index)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["torch.save(cnn.state_dict(), '\\\\net\\\\net_parameters.pkl')"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[('conv1.weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('conv1.bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True)), ('conv2.weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('conv2.bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True)), ('fc1.weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('fc1.bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n"}],"source":["for m in cnn.modules():\n","    params = list(m.named_parameters())\n","    print(params)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}