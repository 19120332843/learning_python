{"cells":[{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"========================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[75], loss: 0.04035392\ntotal:175, accuracy:0.9371428571428572, sum:164, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[76], loss: 0.03908682\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[77], loss: 0.03793562\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[78], loss: 0.03681906\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9923809523809524, sum:521\nEpoch[79], loss: 0.03575398\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9923809523809524, sum:521\nEpoch[80], loss: 0.03466172\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9923809523809524, sum:521\nEpoch[81], loss: 0.03367284\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9923809523809524, sum:521\nEpoch[82], loss: 0.03263215\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[83], loss: 0.03168665\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[84], loss: 0.03068430\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[85], loss: 0.02976295\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[86], loss: 0.02879823\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[87], loss: 0.02788289\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[88], loss: 0.02693636\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[89], loss: 0.02598395\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[90], loss: 0.02505377\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[91], loss: 0.02419938\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[92], loss: 0.02336701\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[93], loss: 0.02259857\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[94], loss: 0.02184660\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[95], loss: 0.02115618\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[96], loss: 0.02048875\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[97], loss: 0.01985998\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[98], loss: 0.01926704\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[99], loss: 0.01868462\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[100], loss: 0.01812234\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[101], loss: 0.01759750\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[102], loss: 0.01711829\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[103], loss: 0.01666833\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[104], loss: 0.01624937\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[105], loss: 0.01585096\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[106], loss: 0.01547206\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[107], loss: 0.01509933\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9942857142857143, sum:522\nEpoch[108], loss: 0.01473503\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[109], loss: 0.01438037\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[110], loss: 0.01403297\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[111], loss: 0.01369277\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[112], loss: 0.01335495\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[113], loss: 0.01302384\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[114], loss: 0.01269232\ntotal:175, accuracy:0.9428571428571428, sum:165, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[115], loss: 0.01237446\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[116], loss: 0.01206653\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[117], loss: 0.01176618\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[118], loss: 0.01147384\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[119], loss: 0.01118436\ntotal:175, accuracy:0.9485714285714286, sum:166, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[120], loss: 0.01090546\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[121], loss: 0.01063378\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[122], loss: 0.01037121\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[123], loss: 0.01011675\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[124], loss: 0.00987962\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[125], loss: 0.00964837\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9961904761904762, sum:523\nEpoch[126], loss: 0.00941587\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[127], loss: 0.00919395\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[128], loss: 0.00898266\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[129], loss: 0.00877707\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[130], loss: 0.00858572\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[131], loss: 0.00840247\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[132], loss: 0.00822384\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[133], loss: 0.00805951\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[134], loss: 0.00789562\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[135], loss: 0.00774281\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[136], loss: 0.00760128\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[137], loss: 0.00746282\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[138], loss: 0.00733339\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[139], loss: 0.00721175\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[140], loss: 0.00709613\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[141], loss: 0.00698790\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[142], loss: 0.00688440\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[143], loss: 0.00678355\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[144], loss: 0.00668679\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[145], loss: 0.00659681\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[146], loss: 0.00651084\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[147], loss: 0.00642782\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[148], loss: 0.00634713\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[149], loss: 0.00627100\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[150], loss: 0.00619800\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[151], loss: 0.00612986\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[152], loss: 0.00606218\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[153], loss: 0.00599865\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[154], loss: 0.00593610\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[155], loss: 0.00587641\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[156], loss: 0.00582061\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[157], loss: 0.00576789\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[158], loss: 0.00571616\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[159], loss: 0.00566829\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\ntotal_train:525, accuracy:0.9980952380952381, sum:524\nEpoch[160], loss: 0.00561938\ntotal:175, accuracy:0.9542857142857143, sum:167, max=0.96, maxepoch=38\n=============================================================================\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import os \n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","def Normlize2(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmin) / (Zmax - Zmin)\n","    return Z\n","\n","\n","def Data_Reading(Normalization=True):\n","    # trainset = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\3\\\\trainset2.npy')\n","    # trainlabel = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\trainlabel2.npy')\n","    # testset = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\trainset2.npy')\n","    # testlabel = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\nos-data\\\\3times7class\\\\trainlabel2.npy')\n","    data = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\codedata\\\\3times\\\\dataset.npy')\n","    label = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\codedata\\\\3times\\\\label.npy')\n","\n","    # Normalization\n","    # trainset = Normlize(trainset)\n","    # testset = Normlize(testset)\n","    data = Normlize(data)\n","\n","    # myself or auto\n","    if Normalization:\n","        train_x = torch.from_numpy(trainset).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(trainlabel).type(torch.int64)#.int64\n","        test_x = torch.from_numpy(testset).type(torch.cuda.FloatTensor)\n","        test_y = torch.from_numpy(testlabel).type(torch.int64)\n","    else:\n","        data = torch.from_numpy(data).type(torch.cuda.FloatTensor)\n","        label = torch.from_numpy(label).type(torch.int64)\n","\n","    # reshape\n","    # train_x = train_x.view(525, 1, 10, 120)\n","    # test_x = test_x.view(175, 1, 10, 120)\n","    data = data.view(700, 1, 10, 120)\n","    \n","    data = data.cpu().numpy()\n","    label = label.numpy()    \n","    train_x, test_x, train_y, test_y = train_test_split(data, label, test_size=0.25) \n","    train_x = torch.from_numpy(train_x).type(torch.cuda.FloatTensor)\n","    test_x = torch.from_numpy(test_x).type(torch.cuda.FloatTensor)\n","    train_y = torch.from_numpy(train_y).type(torch.int64)\n","    test_y = torch.from_numpy(test_y).type(torch.int64)\n","\n","    # permutation = np.random.permutation(train_y.shape[0])\n","    # train_x = train_x[permutation, :, :, :]\n","    # print('---------------------------------------------------------------------------')\n","    # train_y = train_y[permutation]\n","    return train_x, test_x, train_y, test_y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1,6,(3,3),stride=(1,1))#(10 - 3)/1+1=8 :（120 - 3）/1 + 1 = 118 \n","        self.conv2 = nn.Conv2d(6,10,(4,4),stride=(2,1))#(8 - 4)/2+1 = 3 : (118-4)/1+1 = 115\n","        self.fc1 = nn.Linear(10*3*115,7)#427\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.001)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        return x#\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n","\n","    print(device)\n","\n","    cnn = Net()\n","    # print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    lrr = 0.008\n","    optimizer = optim.SGD(cnn.parameters(), lr=lrr, momentum=0.8)#\n","    loss_func = nn.CrossEntropyLoss()#CrossEntropyLoss()\n","\n","    train_x, test_x, train_y, test_y = Data_Reading(Normalization=0)\n","    train_y = train_y.squeeze()\n","    test_y = test_y.squeeze()\n","    train_x = train_x.to(device)\n","    test_x = test_x.to(device)\n","    train_y = train_y.to(device)\n","    test_y = test_y.to(device)\n","\n","    \n","\n","    #train\n","    sum = 0\n","    max = 0\n","    \n","    batch_size = 21\n","    tr_x = Variable(train_x)\n","    tr_y = Variable(train_y)\n","    for epoch in range(160):\n","        running_loss = 0.0\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            out = cnn(t_x)\n","            loss = loss_func(out, t_y)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","   \n","        running_loss = running_loss / 25\n","\n","        out = cnn(tr_x)\n","        predicted_train = torch.max(out.data, 1)[1]\n","        total_train = tr_y.size(0)#总数\n","        for j in range(tr_y.size(0)):\n","            # print('j = {}, predicted:{}, corr:{}'.format(j, predicted_train[j], tr_y[j]))\n","            if predicted_train[j] == tr_y[j]:\n","                sum += 1\n","        \n","        print('total_train:{}, accuracy:{}, sum:{}'.format(total_train, sum / total_train, sum))\n","        sum = 0\n","\n","        if (sum / total_train > 0.92) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=lrr/10, momentum=0.8/4)\n","        elif (sum / total_train > 0.98) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=lrr/10/10, momentum=0)#momentum=0\n","\n","\n","        print('Epoch[{}], loss: {:.8f}'.format(epoch + 1, running_loss))\n","    \n","    \n","    #test\n","        te_x = Variable(test_x)\n","        te_y = Variable(test_y)\n","        out1 = cnn(te_x)\n","        predicted_test = torch.max(out1.data, 1)[1]#.data.squeeze()\n","        total = te_y.size(0)\n","\n","        for j in range(te_y.size(0)):\n","            if predicted_test[j] == te_y[j]:\n","                sum += 1\n","        \n","        if(max < sum/total):\n","            max = sum/total\n","            maxepoch = epoch + 1\n","        print('total:{}, accuracy:{}, sum:{}, max={}, maxepoch={}'.format(total, sum / total, sum, max, maxepoch))\n","        print('=============================================================================')\n","        sum = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[0, 2, 4, 7, 3, 1, 9, 8, 6, 5]\n"}],"source":["import random\n","index = random.sample(range(0,10),10)\n","print(index)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["torch.save(cnn.state_dict(), '\\\\net\\\\net_parameters.pkl')"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[('conv1.weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('conv1.bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True)), ('conv2.weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('conv2.bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True)), ('fc1.weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('fc1.bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n"}],"source":["for m in cnn.modules():\n","    params = list(m.named_parameters())\n","    print(params)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}