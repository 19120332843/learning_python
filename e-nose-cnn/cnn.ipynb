{"cells":[{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"=========================================================================\ntotal_train:410, accuracy:0.275609756097561, sum:113\nEpoch[105], loss: 1.37036716\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2780487804878049, sum:114\nEpoch[106], loss: 1.37000381\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.28292682926829266, sum:116\nEpoch[107], loss: 1.36963128\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.28292682926829266, sum:116\nEpoch[108], loss: 1.36924916\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.28780487804878047, sum:118\nEpoch[109], loss: 1.36885720\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.28780487804878047, sum:118\nEpoch[110], loss: 1.36845540\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.28780487804878047, sum:118\nEpoch[111], loss: 1.36804337\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.28780487804878047, sum:118\nEpoch[112], loss: 1.36762085\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.28780487804878047, sum:118\nEpoch[113], loss: 1.36718770\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.29024390243902437, sum:119\nEpoch[114], loss: 1.36674345\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2926829268292683, sum:120\nEpoch[115], loss: 1.36628809\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2926829268292683, sum:120\nEpoch[116], loss: 1.36582097\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2926829268292683, sum:120\nEpoch[117], loss: 1.36534157\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2975609756097561, sum:122\nEpoch[118], loss: 1.36485042\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2975609756097561, sum:122\nEpoch[119], loss: 1.36434746\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2975609756097561, sum:122\nEpoch[120], loss: 1.36383180\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2975609756097561, sum:122\nEpoch[121], loss: 1.36330324\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.2975609756097561, sum:122\nEpoch[122], loss: 1.36276166\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3024390243902439, sum:124\nEpoch[123], loss: 1.36220630\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3024390243902439, sum:124\nEpoch[124], loss: 1.36163637\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3048780487804878, sum:125\nEpoch[125], loss: 1.36105159\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3073170731707317, sum:126\nEpoch[126], loss: 1.36045156\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3097560975609756, sum:127\nEpoch[127], loss: 1.35983619\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3097560975609756, sum:127\nEpoch[128], loss: 1.35920351\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3146341463414634, sum:129\nEpoch[129], loss: 1.35855401\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3170731707317073, sum:130\nEpoch[130], loss: 1.35788678\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3195121951219512, sum:131\nEpoch[131], loss: 1.35720117\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.32926829268292684, sum:135\nEpoch[132], loss: 1.35649667\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.33414634146341465, sum:137\nEpoch[133], loss: 1.35577291\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3439024390243902, sum:141\nEpoch[134], loss: 1.35502938\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.348780487804878, sum:143\nEpoch[135], loss: 1.35426622\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.37073170731707317, sum:152\nEpoch[136], loss: 1.35348262\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.3853658536585366, sum:158\nEpoch[137], loss: 1.35267758\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.40487804878048783, sum:166\nEpoch[138], loss: 1.35185057\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.4121951219512195, sum:169\nEpoch[139], loss: 1.35100161\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.4219512195121951, sum:173\nEpoch[140], loss: 1.35012980\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.4317073170731707, sum:177\nEpoch[141], loss: 1.34923432\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.43902439024390244, sum:180\nEpoch[142], loss: 1.34831431\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.43658536585365854, sum:179\nEpoch[143], loss: 1.34736881\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.43414634146341463, sum:178\nEpoch[144], loss: 1.34639688\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.44634146341463415, sum:183\nEpoch[145], loss: 1.34539769\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.4560975609756098, sum:187\nEpoch[146], loss: 1.34437010\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.4658536585365854, sum:191\nEpoch[147], loss: 1.34331294\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.47073170731707314, sum:193\nEpoch[148], loss: 1.34222537\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.47073170731707314, sum:193\nEpoch[149], loss: 1.34110573\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.47560975609756095, sum:195\nEpoch[150], loss: 1.33995360\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.47804878048780486, sum:196\nEpoch[151], loss: 1.33876682\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.48048780487804876, sum:197\nEpoch[152], loss: 1.33754394\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.4878048780487805, sum:200\nEpoch[153], loss: 1.33628344\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.4975609756097561, sum:204\nEpoch[154], loss: 1.33498436\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.5024390243902439, sum:206\nEpoch[155], loss: 1.33364458\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.5073170731707317, sum:208\nEpoch[156], loss: 1.33226212\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.5170731707317073, sum:212\nEpoch[157], loss: 1.33083492\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.524390243902439, sum:215\nEpoch[158], loss: 1.32936068\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.5341463414634147, sum:219\nEpoch[159], loss: 1.32783721\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.5365853658536586, sum:220\nEpoch[160], loss: 1.32626189\ntotal:174, accuracy:0.27586206896551724, sum:48\n=============================================================================\ntotal_train:410, accuracy:0.5390243902439025, sum:221\nEpoch[161], loss: 1.32463215\ntotal:174, accuracy:0.28160919540229884, sum:49\n=============================================================================\ntotal_train:410, accuracy:0.5463414634146342, sum:224\nEpoch[162], loss: 1.32294621\ntotal:174, accuracy:0.28160919540229884, sum:49\n=============================================================================\ntotal_train:410, accuracy:0.551219512195122, sum:226\nEpoch[163], loss: 1.32120060\ntotal:174, accuracy:0.28160919540229884, sum:49\n=============================================================================\ntotal_train:410, accuracy:0.551219512195122, sum:226\nEpoch[164], loss: 1.31939233\ntotal:174, accuracy:0.28160919540229884, sum:49\n=============================================================================\ntotal_train:410, accuracy:0.5634146341463414, sum:231\nEpoch[165], loss: 1.31751767\ntotal:174, accuracy:0.28735632183908044, sum:50\n=============================================================================\ntotal_train:410, accuracy:0.5658536585365853, sum:232\nEpoch[166], loss: 1.31557301\ntotal:174, accuracy:0.29310344827586204, sum:51\n=============================================================================\ntotal_train:410, accuracy:0.573170731707317, sum:235\nEpoch[167], loss: 1.31355428\ntotal:174, accuracy:0.3045977011494253, sum:53\n=============================================================================\ntotal_train:410, accuracy:0.573170731707317, sum:235\nEpoch[168], loss: 1.31145778\ntotal:174, accuracy:0.3218390804597701, sum:56\n=============================================================================\ntotal_train:410, accuracy:0.5853658536585366, sum:240\nEpoch[169], loss: 1.30927882\ntotal:174, accuracy:0.3275862068965517, sum:57\n=============================================================================\ntotal_train:410, accuracy:0.5926829268292683, sum:243\nEpoch[170], loss: 1.30701329\ntotal:174, accuracy:0.3448275862068966, sum:60\n=============================================================================\ntotal_train:410, accuracy:0.6097560975609756, sum:250\nEpoch[171], loss: 1.30465580\ntotal:174, accuracy:0.3563218390804598, sum:62\n=============================================================================\ntotal_train:410, accuracy:0.6170731707317073, sum:253\nEpoch[172], loss: 1.30220058\ntotal:174, accuracy:0.3735632183908046, sum:65\n=============================================================================\ntotal_train:410, accuracy:0.6341463414634146, sum:260\nEpoch[173], loss: 1.29964177\ntotal:174, accuracy:0.41379310344827586, sum:72\n=============================================================================\ntotal_train:410, accuracy:0.6487804878048781, sum:266\nEpoch[174], loss: 1.29697291\ntotal:174, accuracy:0.42528735632183906, sum:74\n=============================================================================\ntotal_train:410, accuracy:0.6682926829268293, sum:274\nEpoch[175], loss: 1.29418746\ntotal:174, accuracy:0.4482758620689655, sum:78\n=============================================================================\ntotal_train:410, accuracy:0.6829268292682927, sum:280\nEpoch[176], loss: 1.29127775\ntotal:174, accuracy:0.45977011494252873, sum:80\n=============================================================================\ntotal_train:410, accuracy:0.6902439024390243, sum:283\nEpoch[177], loss: 1.28823618\ntotal:174, accuracy:0.47701149425287354, sum:83\n=============================================================================\ntotal_train:410, accuracy:0.697560975609756, sum:286\nEpoch[178], loss: 1.28505433\ntotal:174, accuracy:0.4827586206896552, sum:84\n=============================================================================\ntotal_train:410, accuracy:0.697560975609756, sum:286\nEpoch[179], loss: 1.28172255\ntotal:174, accuracy:0.4827586206896552, sum:84\n=============================================================================\ntotal_train:410, accuracy:0.6902439024390243, sum:283\nEpoch[180], loss: 1.27823133\ntotal:174, accuracy:0.4885057471264368, sum:85\n=============================================================================\ntotal_train:410, accuracy:0.6878048780487804, sum:282\nEpoch[181], loss: 1.27456901\ntotal:174, accuracy:0.4885057471264368, sum:85\n=============================================================================\ntotal_train:410, accuracy:0.6829268292682927, sum:280\nEpoch[182], loss: 1.27072455\ntotal:174, accuracy:0.4827586206896552, sum:84\n=============================================================================\ntotal_train:410, accuracy:0.6682926829268293, sum:274\nEpoch[183], loss: 1.26668483\ntotal:174, accuracy:0.4885057471264368, sum:85\n=============================================================================\ntotal_train:410, accuracy:0.6609756097560976, sum:271\nEpoch[184], loss: 1.26243580\ntotal:174, accuracy:0.4942528735632184, sum:86\n=============================================================================\ntotal_train:410, accuracy:0.6487804878048781, sum:266\nEpoch[185], loss: 1.25796247\ntotal:174, accuracy:0.4942528735632184, sum:86\n=============================================================================\ntotal_train:410, accuracy:0.6317073170731707, sum:259\nEpoch[186], loss: 1.25324978\ntotal:174, accuracy:0.4942528735632184, sum:86\n=============================================================================\ntotal_train:410, accuracy:0.6341463414634146, sum:260\nEpoch[187], loss: 1.24827956\ntotal:174, accuracy:0.4942528735632184, sum:86\n=============================================================================\ntotal_train:410, accuracy:0.6292682926829268, sum:258\nEpoch[188], loss: 1.24303574\ntotal:174, accuracy:0.4885057471264368, sum:85\n=============================================================================\ntotal_train:410, accuracy:0.6121951219512195, sum:251\nEpoch[189], loss: 1.23749618\ntotal:174, accuracy:0.4885057471264368, sum:85\n=============================================================================\ntotal_train:410, accuracy:0.6121951219512195, sum:251\nEpoch[190], loss: 1.23163927\ntotal:174, accuracy:0.4827586206896552, sum:84\n=============================================================================\ntotal_train:410, accuracy:0.6073170731707317, sum:249\nEpoch[191], loss: 1.22544268\ntotal:174, accuracy:0.4827586206896552, sum:84\n=============================================================================\ntotal_train:410, accuracy:0.6073170731707317, sum:249\nEpoch[192], loss: 1.21887961\ntotal:174, accuracy:0.4885057471264368, sum:85\n=============================================================================\ntotal_train:410, accuracy:0.6097560975609756, sum:250\nEpoch[193], loss: 1.21192746\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\ntotal_train:410, accuracy:0.6121951219512195, sum:251\nEpoch[194], loss: 1.20455315\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\ntotal_train:410, accuracy:0.6121951219512195, sum:251\nEpoch[195], loss: 1.19672965\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\ntotal_train:410, accuracy:0.6146341463414634, sum:252\nEpoch[196], loss: 1.18842326\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\ntotal_train:410, accuracy:0.6170731707317073, sum:253\nEpoch[197], loss: 1.17959824\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\ntotal_train:410, accuracy:0.6219512195121951, sum:255\nEpoch[198], loss: 1.17022298\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\ntotal_train:410, accuracy:0.6292682926829268, sum:258\nEpoch[199], loss: 1.16025835\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\ntotal_train:410, accuracy:0.6390243902439025, sum:262\nEpoch[200], loss: 1.14966490\ntotal:174, accuracy:0.5, sum:87\n=============================================================================\n"}],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import os \n","\n","\n","def Normlize(Z):\n","    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n","    Zmean = Z.mean(axis=1)\n","    #按列排序\n","    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n","    Zmean = Zmean.reshape(-1, 1)\n","    Z = (Z - Zmean) / (Zmax - Zmin)\n","    return Z\n","\n","\n","def Data_Reading(Normalization=True):\n","    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainset2.npy')\n","    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\train\\\\trainlabel2.npy')\n","    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testset2.npy')\n","    test_y = np.load('F:\\\\gitworkspace\\\\python\\\\e-nose-cnn\\\\test\\\\testlabel2.npy')\n","\n","    # Normalization\n","    train_x_Normed = Normlize(train_x)\n","    test_x_Normed = Normlize(test_x)\n","\n","    # xlsx to tensor\n","    if Normalization:\n","        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n","        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.int64)\n","        test_y = torch.from_numpy(test_y).type(torch.int64)\n","\n","    else:\n","        train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n","        test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n","        train_y = torch.from_numpy(train_y).type(torch.int64)\n","        test_y = torch.from_numpy(test_y).type(torch.int64)\n","    # reshape\n","    train_x = train_x.view(410, 1, 10, 120)\n","    test_x = test_x.view(174, 1, 10, 120)\n","    # train_x = train_x.view(408, 1, 10, 120)\n","    # test_x = test_x.view(202, 1, 10, 120)\n","\n","    permutation = np.random.permutation(train_y.shape[0])\n","    train_x = train_x[permutation, :, :, :]\n","    print('---------------------------------------------------------------------------')\n","    train_y = train_y[permutation]\n","    print(train_x[6])\n","    return train_x, test_x, train_y, test_y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # self.conv1 = nn.Conv2d(1,3,(1,3),stride=(1,1))#(10 - 1)/1 + 1 = 10   (120 - 3)/1 + 1 = 118 118/2 = 59\n","        # self.conv2 = nn.Conv2d(3,6,(2,3),stride=(2,1))#(10 - 2)/2 + 1 = 5   (59 - 3)/1 + 1 = 57 \n","        # self.conv3 = nn.Conv2d(6,10,(5,3),stride=(1,1))# (5 - 5)/1 + 1 = 1  (57 - 3)/1 + 1 = 55\n","        # self.fc1 = nn.Linear(10*1*55,4)\n","\n","        self.conv1 = nn.Conv2d(1,2,(1,4),stride=(1,2))#(10 - 1)/1 + 1 = 10   (120 - 4)/2 + 1 = 59 118/2 = 59\n","        self.conv2 = nn.Conv2d(2,4,(2,3),stride=(2,1))#(10 - 2)/2 + 1 = 5   (59 - 3)/1 + 1 = 57 \n","        self.conv3 = nn.Conv2d(4,6,(3,3),stride=(2,1))# (5 - 3)/2 + 1 = 2  (57 - 3)/1 + 1 = 55\n","        self.conv4 = nn.Conv2d(6,8,(2,3),stride=(1,1))#1  (55 - 3)/1 + 1 = 53\n","        self.fc1 = nn.Linear(8*1*53,4)\n","\n","       \n","        # self.conv1 = nn.Conv2d(1,6,(10,4),stride=(1,2))#59\n","        # self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,1))#57      \n","        # self.fc1 = nn.Linear(10*1*57,4)\n","\n","        # self.fc2 = nn.Linear(342,4)\n","        # self.fc3 = nn.Linear(85,4)\n","        # self.fc4 = nn.Linear(64,4)\n","        \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.xavier_uniform_(m.weight)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.001)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = x.view(x.size(0), -1)\n","        # x = F.logsigmoid(self.fc1(x))\n","        # x = F.logsigmoid(self.fc2(x))\n","        # x = F.relu(self.fc3(x))\n","        x = self.fc1(x)\n","        x = F.log_softmax(x,dim=1)\n","        # return x\n","        return x#\n","\n","\n","if __name__ == '__main__':\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#cuda:0\n","\n","    print(device)\n","\n","    cnn = Net()\n","    print(cnn)\n","    cnn.to(device)\n","\n","    #sgd -> stochastic gradient descent\n","    optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.6)#\n","    loss_func = nn.NLLLoss()#CrossEntropyLoss()\n","\n","    train_x, test_x, train_y, test_y = Data_Reading(Normalization=1)\n","    train_y = train_y.squeeze()\n","    test_y = test_y.squeeze()\n","    train_x = train_x.to(device)\n","    test_x = test_x.to(device)\n","    train_y = train_y.to(device)\n","    test_y = test_y.to(device)\n","\n","    \n","\n","    #train\n","    sum = 0\n","\n","    batch_size = 17\n","    step = 10\n","    tr_x = Variable(train_x)\n","    tr_y = Variable(train_y)\n","    for epoch in range(200):\n","        running_loss = 0.0\n","        for i in range(0,(int)(len(train_x)/batch_size)):\n","            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n","            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n","            out = cnn(t_x)\n","            # print(out)\n","            #forward\n","            loss = loss_func(out, t_y)\n","            #梯度初始化为零\n","            optimizer.zero_grad()\n","             \n","            #backward\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            # print('loss = {}, i = {}'.format(loss.item(), i))\n","\n","            # predicted1 = torch.max(out1.data, 1)[1]\n","   \n","        running_loss = running_loss / 24\n","\n","        out = cnn(tr_x)\n","        predicted_train = torch.max(out.data, 1)[1]\n","        total_train = tr_y.size(0)\n","        for j in range(tr_y.size(0)):\n","            if predicted_train[j] == tr_y[j]:\n","                sum += 1\n","        \n","        print('total_train:{}, accuracy:{}, sum:{}'.format(total_train, sum / total_train, sum))\n","        sum = 0\n","\n","        if (sum / total_train > 0.85) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.3)\n","        elif (sum / total_train > 0.95) :\n","            optimizer = optim.SGD(cnn.parameters(), lr=0.00001, momentum=0)\n","\n","\n","        print('Epoch[{}], loss: {:.8f}'.format(epoch + 1, running_loss))\n","    \n","    \n","    #test\n","        te_x = Variable(test_x)\n","        te_y = Variable(test_y)\n","        out1 = cnn(te_x)\n","        predicted_test = torch.max(out1.data, 1)[1]#.data.squeeze()\n","        total = te_y.size(0)\n","\n","        for j in range(te_y.size(0)):\n","            if predicted_test[j] == te_y[j]:\n","                sum += 1\n","        print('total:{}, accuracy:{}, sum:{}'.format(total, sum / total, sum))\n","        print('=============================================================================')\n","        sum = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[0, 2, 4, 7, 3, 1, 9, 8, 6, 5]\n"}],"source":["import random\n","index = random.sample(range(0,10),10)\n","print(index)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["torch.save(cnn.state_dict(), '\\\\net\\\\net_parameters.pkl')"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[('conv1.weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('conv1.bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True)), ('conv2.weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('conv2.bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True)), ('fc1.weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('fc1.bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[-0.3761,  0.1001, -0.4186, -0.1839]],\n\n         [[-0.2533, -0.4018, -0.4310, -0.5048]],\n\n         [[-0.2632, -0.3185,  0.1013, -0.0373]],\n\n         [[-0.4070, -0.1267, -0.3912, -0.4335]],\n\n         [[ 0.0283,  0.0544, -0.2606, -0.4131]],\n\n         [[ 0.1456,  0.0818,  0.0535, -0.2685]],\n\n         [[-0.2558, -0.1103,  0.2508,  0.1695]],\n\n         [[ 0.0783,  0.1154,  0.0144, -0.0385]],\n\n         [[ 0.4976,  0.2357,  0.4006,  0.4427]],\n\n         [[ 0.2270, -0.0259, -0.1455, -0.0803]]],\n\n\n        [[[-0.4097, -0.2975, -0.1239, -0.0020]],\n\n         [[-0.1164, -0.2830,  0.0770, -0.1116]],\n\n         [[ 0.1473, -0.4038, -0.2259, -0.3002]],\n\n         [[-0.0664, -0.2939, -0.2580, -0.0312]],\n\n         [[-0.2715,  0.0019, -0.0976, -0.1689]],\n\n         [[-0.2187, -0.1226,  0.1654,  0.1052]],\n\n         [[-0.3563,  0.1089,  0.1260, -0.1577]],\n\n         [[ 0.2518, -0.2247,  0.2262, -0.1033]],\n\n         [[ 0.1777,  0.5143,  0.2523,  0.4627]],\n\n         [[-0.0060,  0.1343, -0.0585,  0.1960]]],\n\n\n        [[[ 0.3278,  0.0861, -0.1483,  0.2228]],\n\n         [[ 0.1350,  0.2892, -0.2363, -0.0062]],\n\n         [[ 0.0282,  0.2710,  0.2895,  0.2370]],\n\n         [[ 0.5029,  0.4682,  0.3331,  0.5553]],\n\n         [[-0.1901, -0.2484,  0.0843,  0.1969]],\n\n         [[-0.0113, -0.0074, -0.3122, -0.2348]],\n\n         [[-0.3502, -0.0204, -0.1498, -0.3732]],\n\n         [[-0.1094, -0.1301,  0.1298, -0.3305]],\n\n         [[ 0.0550, -0.1592,  0.1609, -0.1762]],\n\n         [[ 0.4105,  0.5699,  0.1815,  0.0446]]],\n\n\n        [[[ 0.0526, -0.2687,  0.0715, -0.0765]],\n\n         [[ 0.5235,  0.3908,  0.1531,  0.4271]],\n\n         [[ 0.3552,  0.2256, -0.1776,  0.0712]],\n\n         [[ 0.2257,  0.0348,  0.4382, -0.0079]],\n\n         [[-0.3437, -0.4817, -0.0339, -0.1791]],\n\n         [[-0.6845, -0.5523, -0.3990, -0.5756]],\n\n         [[ 0.0912, -0.2664, -0.3844, -0.3606]],\n\n         [[-0.0634, -0.1292,  0.3401, -0.1327]],\n\n         [[-0.1310, -0.1898, -0.0786, -0.1869]],\n\n         [[ 0.4655,  0.3949,  0.2616, -0.1908]]],\n\n\n        [[[ 0.2907,  0.1810, -0.0400, -0.2138]],\n\n         [[-0.2099, -0.3451, -0.2970, -0.1598]],\n\n         [[ 0.3817,  0.3231,  0.3387, -0.0720]],\n\n         [[-0.0490, -0.2093,  0.0066, -0.2393]],\n\n         [[-0.0666, -0.1395,  0.0202,  0.2572]],\n\n         [[ 0.2943,  0.0496,  0.1442,  0.1744]],\n\n         [[-0.0092,  0.3358,  0.1301,  0.2684]],\n\n         [[-0.0977, -0.2858,  0.1630,  0.0838]],\n\n         [[-0.0146, -0.1504,  0.2521,  0.0589]],\n\n         [[-0.2481,  0.3461,  0.2580,  0.3978]]],\n\n\n        [[[ 0.0997,  0.2157,  0.0510, -0.2752]],\n\n         [[-0.4123,  0.0668, -0.1229,  0.0036]],\n\n         [[-0.0809, -0.3130, -0.1719,  0.1061]],\n\n         [[-0.0560, -0.1409, -0.0608,  0.0378]],\n\n         [[ 0.3159,  0.0498, -0.1897, -0.2776]],\n\n         [[ 0.0364,  0.2943,  0.4137,  0.3621]],\n\n         [[-0.1622,  0.2010,  0.1531, -0.2668]],\n\n         [[ 0.2236, -0.1850,  0.3025,  0.2402]],\n\n         [[-0.1395, -0.0616,  0.0381, -0.3831]],\n\n         [[ 0.5199,  0.6171,  0.3261,  0.4577]]]], device='cuda:0',\n       requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.1513,  0.0591,  0.1234,  0.0241,  0.3073,  0.0845], device='cuda:0',\n       requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[[[ 0.5542,  0.2779,  0.7145]],\n\n         [[ 0.3094,  0.6137,  0.7147]],\n\n         [[ 0.5674,  0.6161,  0.3855]],\n\n         [[ 0.5847,  0.5378, -0.0951]],\n\n         [[ 0.1982, -0.3951,  0.2889]],\n\n         [[ 0.3455, -0.0670,  0.3766]]],\n\n\n        [[[-0.5999, -0.4073, -0.1099]],\n\n         [[ 0.1796, -0.4108, -0.0431]],\n\n         [[ 0.2072,  0.2256,  0.2694]],\n\n         [[-0.3818, -0.3460, -0.3441]],\n\n         [[ 0.3749,  0.3931,  0.4089]],\n\n         [[ 0.2242, -0.2043,  0.3373]]],\n\n\n        [[[ 0.4278,  0.2063,  0.0404]],\n\n         [[-0.1176, -0.2056,  0.2907]],\n\n         [[-0.4772, -0.0164, -0.3684]],\n\n         [[ 0.1646, -0.0242,  0.4111]],\n\n         [[ 0.3542,  0.0419,  0.3532]],\n\n         [[-0.2703,  0.2126, -0.2713]]],\n\n\n        [[[ 0.2010, -0.1527,  0.1134]],\n\n         [[ 0.1340,  0.0087, -0.0470]],\n\n         [[-0.2304,  0.3520, -0.2616]],\n\n         [[-0.2594, -0.1261, -0.2040]],\n\n         [[-0.2326, -0.3031, -0.1433]],\n\n         [[ 0.1389,  0.5240,  0.4690]]],\n\n\n        [[[ 0.2294,  0.1052, -0.2338]],\n\n         [[ 0.1489, -0.2209, -0.0537]],\n\n         [[ 0.2149, -0.0669,  0.0037]],\n\n         [[-0.0527,  0.1676, -0.1034]],\n\n         [[-0.2351, -0.1799, -0.2872]],\n\n         [[-0.1910,  0.1711, -0.3310]]],\n\n\n        [[[ 0.3510,  0.0850,  0.3798]],\n\n         [[ 0.4153, -0.0173, -0.1575]],\n\n         [[-0.2116,  0.2113,  0.2193]],\n\n         [[ 0.4485,  0.2352,  0.4993]],\n\n         [[ 0.2551, -0.0350, -0.1931]],\n\n         [[-0.2965, -0.4464, -0.3077]]],\n\n\n        [[[ 0.0986,  0.3454, -0.2652]],\n\n         [[-0.0245,  0.3808, -0.2466]],\n\n         [[ 0.2264,  0.4120,  0.4254]],\n\n         [[ 0.2709,  0.0065, -0.1746]],\n\n         [[ 0.1154,  0.1854, -0.1732]],\n\n         [[-0.4264, -0.4442, -0.0378]]],\n\n\n        [[[ 0.1635, -0.1619, -0.0400]],\n\n         [[ 0.0505, -0.0586,  0.1863]],\n\n         [[ 0.0229,  0.3253,  0.2197]],\n\n         [[ 0.3232,  0.0438,  0.4484]],\n\n         [[ 0.0134, -0.2985, -0.3162]],\n\n         [[ 0.0434, -0.3407,  0.0816]]],\n\n\n        [[[ 0.4069,  0.3207, -0.0921]],\n\n         [[ 0.0717,  0.2648, -0.0314]],\n\n         [[ 0.3355,  0.3732, -0.0894]],\n\n         [[ 0.4883,  0.0997,  0.4742]],\n\n         [[-0.1664,  0.0670, -0.2022]],\n\n         [[-0.1044, -0.2206, -0.5075]]],\n\n\n        [[[-0.0611,  0.0222, -0.1183]],\n\n         [[-0.2767,  0.3015, -0.3071]],\n\n         [[ 0.1399, -0.0860, -0.1280]],\n\n         [[-0.0954, -0.3101, -0.3526]],\n\n         [[-0.2372,  0.0524,  0.1198]],\n\n         [[ 0.3321, -0.2208, -0.0204]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-3.7621e-01,  5.4747e-01,  8.4503e-03,  2.4726e-01,  4.3173e-04,\n         4.0693e-02,  3.1866e-02,  1.2527e-01,  8.1584e-02, -1.6725e-03],\n       device='cuda:0', requires_grad=True))]\n[('weight', Parameter containing:\ntensor([[-0.4919,  0.0477,  0.1075,  ..., -0.0100,  0.0113, -0.0100],\n        [-0.4341, -0.2345, -0.1904,  ...,  0.0022, -0.0058,  0.0026],\n        [ 0.0013, -0.2422, -0.1750,  ...,  0.0021,  0.0106,  0.0055],\n        [ 0.9170,  0.4241,  0.1958,  ..., -0.0124, -0.0031, -0.0116]],\n       device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.3562,  0.1960,  0.0529,  0.0613], device='cuda:0',\n       requires_grad=True))]\n"}],"source":["for m in cnn.modules():\n","    params = list(m.named_parameters())\n","    print(params)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}