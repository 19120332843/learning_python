{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"    [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 2 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 3 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 4 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 5 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 6 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 7 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 8 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 9 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\nepoch, loss2:, out2 10 tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward>) tensor([[13702.0205],\n        [13684.2461],\n        [13710.9395],\n        [13729.4414],\n        [13737.3809],\n        [13515.8564],\n        [13396.6230],\n        [13422.8398],\n        [13425.7959],\n        [13599.3623],\n        [13562.6533],\n        [13563.6426],\n        [13560.0801],\n        [13555.9414],\n        [13585.7363],\n        [13545.9951],\n        [13497.8457],\n        [13513.7578],\n        [13389.2754],\n        [13845.4150],\n        [13803.5850],\n        [13794.9277],\n        [13850.5049],\n        [13290.7148],\n        [13213.6846],\n        [13173.0791],\n        [13183.6650],\n        [13195.4268],\n        [13984.7705],\n        [14029.2021],\n        [13655.0430],\n        [14455.0869],\n        [14024.8467],\n        [14597.7617],\n        [14561.0234],\n        [14564.4189],\n        [14581.1318],\n        [13723.6904],\n        [13669.9365],\n        [13612.7832],\n        [13618.1924],\n        [13612.6943],\n        [12595.9824],\n        [12582.5859],\n        [12782.8623],\n        [12559.5225],\n        [12810.7764],\n        [11939.9785],\n        [11981.7500],\n        [11794.8535],\n        [11975.9678],\n        [11978.3350],\n        [12142.8633],\n        [11946.3828],\n        [11950.5098],\n        [11975.2295],\n        [11933.7354],\n        [14247.4219],\n        [14722.6768],\n        [14724.9033],\n        [14740.2949],\n        [14099.2852],\n        [14010.1455],\n        [13975.1885],\n        [11658.2051],\n        [11619.5771],\n        [11531.4648],\n        [11512.3584],\n        [11483.3809],\n        [12597.1846],\n        [12595.8398],\n        [12462.5156],\n        [12543.5781],\n        [12564.1699],\n        [13262.0762],\n        [13159.7832],\n        [13130.7451],\n        [13141.9346],\n        [11979.4131],\n        [12358.7588],\n        [12397.9971],\n        [12360.8779],\n        [12379.9287],\n        [12356.0859],\n        [12279.6611],\n        [12281.6299],\n        [12195.7246],\n        [12245.9863],\n        [12410.8652],\n        [12355.7080],\n        [12332.5801],\n        [12463.5723],\n        [12489.4395],\n        [13666.4102],\n        [13456.9551]], device='cuda:0', grad_fn=<MulBackward0>)\n"}],"source":"import pandas as pd\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n\ndef Normlize(Z):\n    Zmax, Zmin = Z.max(axis=1), Z.min(axis=1)\n    Zmean = Z.mean(axis=1)\n    #按列排序\n    Zmax, Zmin = Zmax.reshape(-1, 1), Zmin.reshape(-1, 1)\n    Zmean = Zmean.reshape(-1, 1)\n    Z = (Z - Zmean) / (Zmax - Zmin)\n    return Z\n\ndef Data_Reading(Normalization=True):\n    # Read the xlsx\n    '''\n    //读取excel的值\n    train_x = pd.read_excel( \"trainingset.xlsx\", 'Input',header = None)\n    test_x = pd.read_excel(\"oilsset.xlsx\",'oils',header = None)\n    test_z = pd.read_excel(\"newodorset.xlsx\",'new',header = None)\n    train_y = pd.read_excel(\"trainingy.xlsx\",'Output',header = None)\n    testy1 = pd.read_excel(\"oilsy.xlsx\",'oy',header = None)\n    testy2 = pd.read_excel(\"newy.xlsx\",'ny',header = None)\n    \n    //使用前一个观察值填充 \n    train_x = train_x.fillna(method='ffill')\n    test_x = test_x.fillna(method='ffill')\n    text_z = test_z.fillna(method='ffill')\n    train_y = train_y.fillna(method='ffill')\n\n    np.save('trainingset.npy', train_x)\n    np.save('oilsset.npy',test_x)\n    np.save('newodorset.npy', test_z)\n    np.save('trainingy.npy', train_y)\n    np.save('testy1.npy', testy1)\n    np.save('testy2.npy', testy2)\n    '''\n    train_x = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\trainingset.npy')\n    test_x = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\oilsset.npy')\n    test_z = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\newodorset.npy')\n    train_y = np.load('F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\trainingy.npy')\n    testy1 = np.load(\"F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\testy1.npy\")\n    testy2 = np.load(\"F:\\\\gitworkspace\\\\python\\\\pop-cnn\\\\testy2.npy\")\n \n\n    # Normalization\n    train_x_Normed = Normlize(train_x)\n    test_x_Normed = Normlize(test_x)\n    test_z_Normed = Normlize(test_z)\n    train_y = train_y / 10000\n    testy1 = testy1 / 10000\n    testy2 = testy2 / 10000\n\n    # xlsx to tensor\n    if Normalization:\n        train_x = torch.from_numpy(train_x_Normed).type(torch.cuda.FloatTensor)\n        test_x = torch.from_numpy(test_x_Normed).type(torch.cuda.FloatTensor)\n        test_z = torch.from_numpy(test_z_Normed).type(torch.cuda.FloatTensor)\n        train_y = torch.from_numpy(train_y).type(torch.cuda.FloatTensor)\n        testy1 = torch.from_numpy(testy1).type(torch.cuda.FloatTensor)\n        testy2 = torch.from_numpy(testy2).type(torch.cuda.FloatTensor)\n\n    else:\n        train_x = torch.from_numpy(train_x).type(torch.cuda.FloatTensor)\n        test_x = torch.from_numpy(test_x).type(torch.cuda.FloatTensor)\n        test_z = torch.from_numpy(test_z).type(torch.cuda.FloatTensor)\n        train_y = torch.from_numpy(train_y).type(torch.cuda.FloatTensor)\n        testy1 = torch.from_numpy(testy1).type(torch.cuda.FloatTensor)\n        testy2 = torch.from_numpy(testy2).type(torch.cuda.FloatTensor)\n\n\n    # reshape\n    train_x = train_x.view(238, 1, 16, 250)\n    test_x = test_x.view(108, 1, 16, 250)\n    test_z = test_z.view(95, 1, 16, 250)\n    return train_x, test_x, test_z, train_y,testy1,testy2\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1,6,(16,4),stride=(1,3))\n        self.conv2 = nn.Conv2d(6,10,(1,3),stride=(1,2))\n        #self.conv3 = nn.Conv2d(10,14,(1,4),stride=(1,2))\n        self.fc = nn.Linear(10*1*41,1)\n        \n\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.xavier_uniform_(m.weight)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.01)\n\n\n             \n    def forward(self, x):\n        x = F.relu(self.conv1(x))#（238,6,1,124）\n        x = F.relu(self.conv2(x))#（238,16,1,23）\n        #x = F.relu(self.conv3(x))\n        x = x.view(x.size(0), -1) # flatten the tensor\n        x = self.fc(x)\n        return x\n\n#training\nbatch_size = 14\ndef train(train_x,train_y,step=20):\n    for epoch in range(160):\n        for i in range(0,(int)(len(train_x)/batch_size)):\n            t_x = Variable(train_x[i*batch_size:i*batch_size+batch_size])\n            t_y = Variable(train_y[i*batch_size:i*batch_size+batch_size])\n            t_x = t_x.to(device)\n            t_y = t_y.to(device)\n            out = cnn(t_x)\n            #forward\n            #loss_func = nn.MSELoss() 均方损失函数  loss(x(i),y(i)) = (x(i) - y(i))^2\n            loss = loss_func(out, t_y)\n            #梯度初始化为零\n            optimizer.zero_grad()\n             \n            #backward\n            loss.backward()\n            optimizer.step()\n        if (epoch + 1) % step == 0:\n            print('Epoch[{}/{}], loss: {:.12f},'.format(epoch + 1,160, loss.item()))\n        \n            \n#predicting\ndef predict(test_x,testy1):\n    for epoch in range(30):\n        te_x = Variable(test_x)\n        tey1 = Variable(testy1)\n        out1 = cnn(te_x)\n        loss1 = loss_func(out1, tey1)\n        out1 = out1 * 10000\n        #print('Epoch[{}/{}], loss1: {:.12f},'.format(epoch + 1, 30, loss1.item()))\n        print('epoch, loss1:, out1', epoch + 1, loss1, out1)\n\ndef newodor(test_z,testy2):\n    for epoch in range(10):\n        te_z = Variable(test_z)\n        tey2 = Variable(testy2)\n        out2 = cnn(te_z)\n        loss2 = loss_func(out2, tey2)\n        out2 = out2 * 10000\n        #print('Epoch[{}/{}], loss2: {:.12f},'.format(epoch + 1, 10, loss2.item()))\n        print('epoch, loss2:, out2', epoch + 1, loss2, out2)\n\n\n\n\nif __name__ == '__main__':\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n    \n    cnn = Net()\n    print(cnn)\n    cnn.to(device)\n\n    #sgd -> stochastic gradient descent\n    optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.8)\n    loss_func = nn.MSELoss()\n\n    train_x, test_x, test_z, train_y,testy1,testy2 = Data_Reading(Normalization=True)\n    train(train_x, train_y, step=20)\n    predict(test_x,testy1)\n    newodor(test_z,testy2)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}